---
title: "Revealing the Personal Effects of Nutrition with Mixed-Effect Bayesian Network"
author:
- Jari Turkia, jari.turkia@cgi.com
- University of Eastern Finland
bibliography: biblio.bib
output:
  html_document: default
  pdf_document: default
abstract: This notebook describes the implementation of the modelling method and the experimental analysis that is described in the main article.
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE, fig.align="center")

# this allows using tikz rendering for plots with "dev=tikz"
knit_hooks$set(plot = function(x, options) {
  if ('tikz' %in% options$dev && !options$external) {
    hook_plot_tex(x, options)
  } else hook_plot_md(x, options)
})

# Load common MEBN package
source("mebn/MEBN.r")
```

In this work we propose a Bayesian network as an appealling way to model and predict the effects of nutrition. The Bayesian network is a directed graphical model where nodes of the graph are random variables and the edges between the nodes indicate a flow of information. In a nutritional modelling we assing the amount of nutrients at the person's diet and the indicators of person's well-being those random variables with a probability distribution of possible values. 

The goal of graphical modelling is to find a graph that most probably describes the correct conditional relationships between nutrients and their effects. In this article we use this method to analyze a dataset from the Sysdimet study [] that contains repeated measurements of 17 nutrients, some basic information about patients (gender, medication) and blood test results. To ease the graph search, we focus only on two-level, bipartite, graphs where nutrients and personal details are assumed to affect blood tests, and only the magnitude of effect is left to be estimated.

**Formal definition of the problem.** Let us denote the graph of interconnected nutrients and responses with $G$. We can then formulate the modeling problem as finding the graph $G$ that is the most probable given the data $D$

\begin{align}
{P}({G}|{D})
\end{align}

By using the Bayes' Rule we can be split this probability into proportions of the data likelihood of the given graph and any prior information we might have about suitable graphs 

\begin{align}
\label{prop_bayes_theorem}
{P}({G}|{D}) \propto {P}({D}|{G}) {P}({G})
\end{align}

Now the problem is converted into a search of the graph with a maximum likelihood for the given data. To enforce our assumption of biologically plausible graphs, we set the graph prior \(P(G)\) to zero for all other graphs than the previously descibed bipartite graphs with nutrients affecting the bodily responses.

The joint probability of the graph \(P(G|D)\) can furthermore factorized into separate local probability distributions [@Bae2016, @Koller:2009:PGM:1795555] by their \textit{Markov blankets}. These probability distributions can be estimated separately and the novelity of our approach lays in their hierarchical estimation.

**Estimation of the local distributions**.
As the data consists of repeated measurements from the patients..

Generally, the local probability distributions can be from exponential family of distributions, but in this case we consider only normally distributed response variables. The subset of parent variables, that we assume containing personal variance, is denoted with $pa_Z(X_i)$. For the mixed-effect modeling we need to estimate parameters \(\phi_i = \{\beta_i, b_i\}\) for expressing the typical and personal reaction types. In a multivariate normal model the uncertainty is furthermore defined by variance-covariance matrix $V_i$

\begin{align}
\begin{split}
\label{Normal LME}
{P}({X_i}|{pa(X_i), \phi_i, G_i}) = EXP-FAM({X_i} | pa(X_i)\beta_i + pa_Z(X_i)b_i, \sigma^2)
\end{split}
\end{align}

This allows us to define the problem of nutritional effects as mixed-effect Bayesian network as defined in [@Bae2016].

The goal of graphical modelling is then to find a graph that most probably describes the correct conditional relationships between nutrients and their effects. At the following analysis we have a dataset with 17 nutrients, some basic information about patients (gender, medication) and blood test results. To ease the graph search, we focus only on graphs where nutrients are affecting blood tests and only the magnitude of effect is left to be estimated. --Multiple levels of magnitude-- 

## Data

The dataset in our experiment comes from Sysdimet study. ... variables, data points..

We have collected our prior information at a separate "Data description.csv"-file. For guiding the search for plausible graphs we have indicated which variables are responses at the graph and which are possible predictors affecting them.

```{r load_data, echo=FALSE, message=FALSE}

# Read the data description
datadesc <- read.csv(file="Data description.csv", header = TRUE, sep = ";")

# Read the actual data matching the description
sysdimet <- read.csv(file="data/SYSDIMET_diet.csv", sep=";", dec=",")

# Define how to iterate through the graph
assumedpredictors <- datadesc[datadesc$Order==100,]    
assumedtargets <- datadesc[datadesc$Order==200,] 
```

**Constructing the graph**

```{r}
initial_normal_graph <- mebn.new_graph_with_randomvariables(datadesc)
```

Edges of the graph are absolute effects of nutrients.

We start from a hierarchical model with normal distribution that estimates of absolute effects are considered as reference when developing the model.

TODO: Plot tikz graph with Gaussian RVs and fully connected edges? No weights yet.


```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
sysdimet_normal <- mebn.bipartite_model(reaction_graph = initial_normal_graph, 
                                         inputdata = sysdimet,
                                         predictor_columns = assumedpredictors, 
                                         assumed_targets = assumedtargets, 
                                         group_column = "SUBJECT_ID",
                                         local_estimation = mebn.sampling,
                                         local_model_cache = "models/BLMM_normal", 
                                         stan_model_file = "mebn/BLMM_normal.stan",
                                         normalize_values = TRUE)

```

**Normal model as reference**

If we now check these initial models of personal blood test responses, we can see that the Gaussian distribution is not a good fit as it allows negative blood test values and does not model the right tail of the true distribution. The estimated model parameters of Gaussian models are still kept as a reference point for further models. Those parameters are the weights of the edges at the Bayesian Network and it is important that all the random variables at the network can work with the same regression coefficients.

```{r normal_model_ppc, echo=FALSE, eval=TRUE}
source("mebn/MEBN.r")
normal_targets <- assumedtargets
normal_targets$ScaleMin <- -10
normal_targets$ScaleMax <- 50

mebn.target_dens_overlays("BLMM_normal/", normal_targets, sysdimet)
```

**Developing the model**

- Biometric data is usually more multiplicative than additive. This is why log-normal distribution makes sense.
  Log-normal / Gamma

```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
initial_gamma_graph <- mebn.new_graph_with_randomvariables(datadesc)

sysdimet_gamma <- mebn.bipartite_model(reaction_graph = initial_gamma_graph, 
                                   inputdata = sysdimet,
                                   predictor_columns = assumedpredictors, 
                                   assumed_targets = assumedtargets, 
                                   group_column = "SUBJECT_ID",
                                   local_estimation = mebn.sampling,
                                   local_model_cache = "models/BLMM_gamma/hierarchical_idlink", 
                                   stan_model_file = "mebn/BLMM_gamma_hierarchical.stan",
                                   normalize_values = TRUE)
```

## Checking the fit of the local models

```{r gamma_ppc, echo=FALSE, eval=TRUE}
mebn.target_dens_overlays("BLMM_gamma/hierarchical_idlink/", assumedtargets, sysdimet)
```

```{r effect_comparison}

# TODO: Make function: mebn.compare_typicals(bn1, bn2) returns this table

# - find beta nodes of both normal and gamma distributions 
normal_nodes <- V(sysdimet_normal)
n_beta <- normal_nodes[normal_nodes$type=="beta"]

gamma_nodes <- V(sysdimet_gamma)
g_beta <- gamma_nodes[gamma_nodes$type=="beta"]

# - construct a table for comparing the estimates
typical_effects<-data.frame(matrix(NA, nrow=length(n_beta), ncol=0))

typical_effects$effect <- unlist(lapply(strsplit(gsub("beta_","", n_beta$name), "_"), function(x) paste0(toString(datadesc[datadesc$Name==x[1],]$Description)," -> ", toString(datadesc[datadesc$Name==x[2],]$Description))))

typical_effects$normal_estimate <- round(n_beta$value,6)
typical_effects$gamma_estimate <- round(g_beta$value,6)

# we see that typical effects are exactly the same in absolute scale
typical_effects

```

**Comparing different local models with LOO**

Besides the visual inspection, we can also compare the models using LOO-PSIS information criteria. It uses log probability that is calculated as part of the Stan models.

```{r, normal_gamma_loo}
normal_vs_gamma <- mebn.LOO_comparison(assumedtargets, "BLMM_normal", "BLMM_gamma/hierarchical_idlink")
normal_vs_gamma
# https://haozhu233.github.io/kableExtra/awesome_table_in_html.html
```


As the expected log predictive density (ELPD) of gamma model is higher, we should choose that for further development.

##Varying response time: AR-structures##

```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
source("mebn/MEBN.r")

initial_graph <- mebn.new_graph_with_randomvariables(datadesc)
sysdimet_gamma_ar1 <- mebn.bipartite_model(reaction_graph = initial_graph, 
                                   inputdata = sysdimet,
                                   predictor_columns = assumedpredictors, 
                                   assumed_targets = assumedtargets, 
                                   group_column = "SUBJECT_ID",
                                   local_estimation = mebn.sampling,
                                   local_model_cache = "models/BLMM_gamma/ar1", 
                                   stan_model_file = "mebn/BLMM_gamma_ar1.stan",
                                   normalize_values = TRUE)

# FSINS and FPLUK-models had undefined values in beta_Intercept and few in C-matrix. Maybe offset-value should be bigger?
```


```{r gamma_ar1_ppc, echo=FALSE, eval=TRUE}
mebn.target_dens_overlays("BLMM_gamma/ar1/", assumedtargets, sysdimet)
```

```{r, gamma_ar1_loo}
ar0_vs_ar1 <- mebn.LOO_comparison(assumedtargets, "BLMM_gamma/hierarchical_idlink", "BLMM_gamma/ar1")
ar0_vs_ar1
```


```{r, ar1}
ar1_comparison <- mebn.AR_comparison(assumedtargets, "BLMM_gamma/ar1")
ar1_comparison
  
```

## Data analysis

Rest of the notebook consideres the inference that we can do with previously estimated mixed-effect bayesian network.

First we do exploration over the whole sample population for finding clusters of similarly behaving patients. Then in the last part we show how we can predict personal nutritional effects for individual patients.

## Principal components as graph visualiation

Local distributions seem to have reasonable good fit. For better overall insight to nutritional effects we can next plot the Bayesian network that describes the connections between nutrients and their responses. (Beta and b nodes in PGM)

- Local distributions are estimated with Stan and values of the parameters are gathered as a Bayesian network
- This enables us to look at the joint probability as a whole
  - Principal components as a graph visualization 
  -- This shows the population effects

```{r, typical_effects_figure, fig.height = 7, fig.width = 7, fig.align = "center"}
source("mebn/MEBN.r")
mebn.plot_typical_effects(sysdimet_gamma_ar1, 15)

```

## Inference

- We can query the graph for all the sigma_b-nodes that represent variation of the effect between persons
- Effects are sorted by magnitude and confidence

```{r, echo=TRUE, eval=TRUE}
# Query the graph for personal variances, denoted by b_sigma-nodes
allnodes <- V(sysdimet_gamma_ar1)
b_sigma <- allnodes[allnodes$type=="b_sigma"]

# Again, a separate data frame is constructed for printing
personal_variances<-data.frame(matrix(NA, nrow=length(b_sigma), ncol=0))

personal_variances$effect <- unlist(lapply(strsplit(gsub("b_sigma_","", b_sigma$name), "_"), function(x) paste0(toString(datadesc[datadesc$Name==x[1],]$Description)," -> ", toString(datadesc[datadesc$Name==x[2],]$Description))))

personal_variances$variance <- b_sigma$value
largest_personal_variance <- personal_variances[order(-personal_variances$variance),]

head(largest_personal_variance, 20)
```

For better undestanding, we can also visualize these personal variances as a graph

```{r, personal_variations_figure, fig.height = 7, fig.width = 7, fig.align = "center"}
source("mebn/MEBN.r")
mebn.plot_personal_variations(sysdimet_gamma_ar1, 15)
```

Although there are personal differences, it is likely that not everyone behave uniquely but there might exist similar groups of behavior. We can analyze personal differences in two ways. We can look the absolute magnitudes of effects and we can also look how persons differ from typical behavior or mean of the effect.

For an effective overview, we gather all personal effects and personal variations from the local distributions of the graph

```{r kmeans_dfs, echo=FALSE, eval=FALSE}
library(rstan)

personal_variations_from_mean <- matrix(NA, ncol=nrow(assumedpredictors)*nrow(assumedtargets), nrow=length(levels(sysdimet$SUBJECT_ID)))
personal_effects <- matrix(NA, ncol=nrow(assumedpredictors)*nrow(assumedtargets), nrow=length(levels(sysdimet$SUBJECT_ID)))
personal_effects_quantiles <- matrix(NA, ncol=3, nrow=106*22*5)

modelcache <- "BLMM_gamma/ar1/"

i <- 1
for (targetname in assumedtargets$Name)
{
  target_blmm <- mebn.get_localfit(paste0(modelcache,targetname))
  posterior <- extract(target_blmm, pars = c("personal_effect", "b"))

  b_blmm <- colMeans(posterior$b)
  beta_b_blmm <- colMeans(posterior$personal_effect)
  
  #pe <- summary(target_blmm, pars="personal_effect", probs = c(0.05, 0.95))$summary[,c(1,4,5)]
  
  if (i == 1) {
    # variance of the intercept is omitted
    personal_variations_from_mean <- b_blmm[,2:dim(b_blmm)[2]]
    personal_effects <- beta_b_blmm
    #personal_effects_quantiles <- pe
  }
  else
  {
    personal_variations_from_mean <- cbind(personal_variations_from_mean, b_blmm[,2:dim(b_blmm)[2]])
    personal_effects <- cbind(personal_effects, beta_b_blmm)
    #personal_effects_quantiles <- rbind(pe)
    i <- i + 1
  }
}

```


## Piileviä reagointitapojen ryhmiä

```{r, echo=FALSE, eval=FALSE}
library(stats)
library(gridExtra)

k.max <- 8
wss_effects <- sapply(1:k.max, function(k){kmeans(personal_effects, k, nstart=50, iter.max = 15)$tot.withinss})
wss_vars <- sapply(1:k.max, function(k){kmeans(personal_variations_from_mean, k, nstart=50, iter.max = 15)$tot.withinss})

df_vars <- data.frame(x = 1:k.max, y = wss_vars)
df_effects <- data.frame(x = 1:k.max, y = wss_effects)

plot1 <- ggplot(data=df_effects, aes(x=x, y=y, group=1)) +
  geom_line() +
  geom_point() + 
  ggtitle("Eri tavoin reagoivia ryhmiä") +
  xlab("Ryhmien määrä") +
  ylab("Ryhmien välinen ero")

plot2 <- ggplot(data=df_vars, aes(x=x, y=y, group=1)) +
  geom_line() +
  geom_point() + 
  ggtitle("Poikkeama tyypillisestä") +
  xlab("Ryhmien määrä") +
  ylab("Ryhmien välinen ero")

gridExtra::grid.arrange(plot1,plot2,nrow=1)
```

### Kuinka tyypillisestä reagointitavasta poiketaan

Tässä kaavion X-akselin nollakohdassa on kyseisen reagointitavan tyypillinen tai keskimääräinen arvo, ja palkit kuvaavat löydettyjen ryhmien poikkeamaa tästä keskiarvosta.

```{r variation clusters, echo=FALSE, eval=FALSE}
km <- kmeans(personal_variations_from_mean, centers = 4)

sysdimet$variation_group <- km$cluster # mihin klusteriin kukin potilas kuuluu. vertaa tätä esimerkiksi geeneihin

variations_data <- as.data.frame(t(km$centers))
variations_data$predictor <- rep(assumedpredictors$Description,nrow(assumedtargets))

cl <- rep(1,22)
t_idx <- c(cl,cl+1,cl+2,cl+3,cl+4)

variations_data$response <- assumedtargets$Description[t_idx]
effect_levels <- paste0(variations_data$predictor," -> ", variations_data$response)

variations_data$effect <- factor(effect_levels, levels=effect_levels)

# Yhteenlaskettu vaikutus kaikissa klustereissa
variations_data$overall_effect <- abs(variations_data$'1')+abs(variations_data$'2')+abs(variations_data$'3')+abs(variations_data$'4')

# Suodata näistä vain suurimmat näkyviin
variations_data.filtered <- variations_data[variations_data$overall_effect > 0.002,]

# Piirtoa varten eri klustereiden arvot samaan sarakkeeseen ja oma kenttä merkitsemään klusteria

plot_data <- variations_data.filtered[c("effect")]
plot_data$amount <- variations_data.filtered$'1'
plot_data$cluster <- 1

temp_data <- variations_data.filtered[c("effect")]
temp_data$amount <- variations_data.filtered$'2'
temp_data$cluster <- 2

plot_data <- rbind(plot_data, temp_data)

temp_data <- variations_data.filtered[c("effect")]
temp_data$amount <- variations_data.filtered$'3'
temp_data$cluster <- 3

plot_data <- rbind(plot_data, temp_data)

temp_data <- variations_data.filtered[c("effect")]
temp_data$amount <- variations_data.filtered$'4'
temp_data$cluster <- 4

plot_data <- rbind(plot_data, temp_data)

plot_data$compared_to_typical <- ifelse(plot_data$amount < 0, "below", "above")

ggplot(plot_data, aes(x=effect, y=amount)) + 
  geom_bar(stat='identity', aes(fill=compared_to_typical), width=.5, show.legend = FALSE) +
  coord_flip() +
  theme(axis.title.x = element_blank(), axis.title.y = element_blank()) +
  facet_wrap(~cluster)

```

### Reagointitapojen ryhmiä

Tässä ryhmittelyä on tehty itse vaikutuksen perusteella.

```{r effect clusters, echo=FALSE, eval=FALSE}
km <- kmeans(personal_effects, centers = 4)

sysdimet$effect_group <- km$cluster # mihin klusteriin kukin potilas kuuluu. vertaa tätä esimerkiksi geeneihin

variations_data <- as.data.frame(t(km$centers))
variations_data$predictor <- rep(assumedpredictors$Description,nrow(assumedtargets))

cl <- rep(1,22)
t_idx <- c(cl,cl+1,cl+2,cl+3,cl+4)

variations_data$response <- assumedtargets$Description[t_idx]
effect_levels <- paste0(variations_data$predictor," -> ", variations_data$response)

variations_data$effect <- factor(effect_levels, levels=effect_levels)

# Yhteenlaskettu vaikutus kaikissa klustereissa
variations_data$overall_effect <- abs(variations_data$'1')+abs(variations_data$'2')+abs(variations_data$'3')+abs(variations_data$'4')

# Suodata näistä vain suurimmat näkyviin
variations_data.filtered <- variations_data[variations_data$overall_effect > 0.004,]

# Piirtoa varten eri klustereiden arvot samaan sarakkeeseen ja oma kenttä merkitsemään klusteria

plot_data <- variations_data.filtered[c("effect")]
plot_data$amount <- variations_data.filtered$'1'
plot_data$cluster <- 1

temp_data <- variations_data.filtered[c("effect")]
temp_data$amount <- variations_data.filtered$'2'
temp_data$cluster <- 2

plot_data <- rbind(plot_data, temp_data)

temp_data <- variations_data.filtered[c("effect")]
temp_data$amount <- variations_data.filtered$'3'
temp_data$cluster <- 3

plot_data <- rbind(plot_data, temp_data)

temp_data <- variations_data.filtered[c("effect")]
temp_data$amount <- variations_data.filtered$'4'
temp_data$cluster <- 4

plot_data <- rbind(plot_data, temp_data)

plot_data$effect_direction <- ifelse(plot_data$amount > 0, "lowers", "raises")

ggplot(plot_data, aes(x=effect, y=amount)) + 
  geom_bar(stat='identity', aes(fill=effect_direction), width=.7, show.legend = FALSE) +
  coord_flip() +
  theme(axis.title.x = element_blank(), axis.title.y = element_blank()) +
  facet_wrap(~cluster)
```


### Personal prediction

Finally, we can hold out a set of data for group of patients and test the predictive ability of the model. Let's calculate the model with a training set and do predictions for this separate test set.

```{r, personal_variations_figure, fig.height = 7, fig.width = 7, fig.align = "center"}
person23 <- mebn.personal_graph(person_id = 23, initial_graph, assumedpredictors, assumedtargets, "BLMM_gamma/ar1/")
mebn.plot_personal_effects(person23, 20)
```



### Henkilökohtaisten ennusteiden varmuus

Kaikkien estimoitujen henkilökohtaisten vaikutusten 95%-luottamusvälit sisältävät nollan. Tässä on kuitenkin esimerkkejä voimakkaimmista. Valitaan joku potilas edellä löydetystä ryhmästä 1 eli keskimäärin voimakkaammin insuliinilla reagoivista.

```{r, echo=FALSE, eval=FALSE}
library(bayesplot)

# Datan visuaalisen tarkastelun perusteella tämän henkilön insuliinitaso nousi selvästi 
# proteiinin saannin myötä

subject_id <- 46   # tumman sininen

protein_id <- match("prot", datadesc$Name)

fsins_blmm <- mebn.get_localfit("fsins")
posterior <- as.array(fsins_blmm)

prot_plot <- mcmc_areas(posterior, pars = paste0("personal_effect[",subject_id,",",protein_id,"]"), prob = 0.50, prob_outer = 0.95, point_est = "mean") + ylab("todennäköisyys") + ggtitle("proteiini - insuliini (S46)") +
  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank())

prot_plot

posterior
```

```{r, echo=FALSE, eval=FALSE}

#sysdimet[sysdimet$effect_group == 1,]$SUBJECT_ID
subject_id <- 46

sakkar_id <- match("sakkar", datadesc$Name)
hhydr_id <- match("hhydr", datadesc$Name)

fsins_blmm <- mebn.get_localfit("fsins")
posterior <- as.array(fsins_blmm)

sakkar_plot <- mcmc_areas(posterior, pars = paste0("personal_effect[",subject_id,",",sakkar_id,"]"), prob = 0.50, prob_outer = 0.95, point_est = "mean") + ylab("todennäköisyys") + ggtitle("sakkaroosi") +
  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank())

hhydr_plot <- mcmc_areas(posterior, pars = paste0("personal_effect[",subject_id,",",hhydr_id,"]"), prob = 0.50, prob_outer = 0.95, point_est = "mean") + ylab("todennäköisyys") + ggtitle("hiilihydraatti") +
  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank())

bayesplot_grid(plots = list(sakkar_plot, hhydr_plot), legends = FALSE)

```









Temporal material.. remove


```{r, echo=FALSE, eval=FALSE}

## Comparing normal and gamma regressions.. absolute parameters should match?

  library(bayesplot)
  library(rstan)

  fshdl_norm <- mebn.get_localfit("BLMM_normal/fshdl")
  fshdl_gamma <- mebn.get_localfit("BLMM_gamma/fshdl")
  fsins_gamma <- mebn.get_localfit("BLMM_gamma/ar1/fsins")
  fsins_ln <- mebn.get_localfit("BLMM_lognormal/fsins")

  fshdl_gamma <- mebn.get_localfit("BLMM_gamma/hierarchical_idlink/fshdl")

  fshdl.true <- sysdimet$fshdl
  fsins.true <- sysdimet$fsins
  fpgluk.true <- sysdimet$fpgluk

  posterior_n <- extract(fshdl_norm, pars = c("Y_rep", "beta", "beta_Intercept", "sigma_e"))
  posterior_g <- extract(fshdl_gamma, pars = c("Y_rep", "beta", "beta_Intercept"))
  posterior_g <- extract(fsins_gamma, pars = c("Y_rep", "beta", "beta_Intercept"))
  posterior_ln <- extract(fsins_ln, pars = c("Y_rep", "beta", "beta_Intercept", "sigma_e"))
  
  mean(posterior_n$beta_Intercept)
  mean(posterior_g$beta_Intercept)
  
  mean(posterior_n$beta[2])
  
  print(fshdl_norm, pars=c("beta_Intercept", "beta", "sigma_b", "sigma_e"))
  print(fsins_ln, pars=c("beta_Intercept", "beta", "sigma_b", "sigma_e"))
  print(fsins_gamma, pars=c("beta_Intercept", "beta", "sigma_b", "sigma_e"))

  posterior_n_50 <- posterior_n$Y_rep[1:50,]
  posterior_ln_50 <- posterior_ln$Y_rep[1:50,]
  posterior_g_50 <- posterior_g$Y_rep[1:50,]
  
  # Back to original scale 
  #fshdl.nrep <- mebn.rescale(posterior_n_50, mean(fshdl.true))
  #fshdl.grep <- mebn.rescale(posterior_g_50, mean(fshdl.true))
  fsins.grep <- mebn.rescale(posterior_g_50, mean(fshdl.true))
  
  ppc_dens_overlay(fshdl.true, posterior_n_50) + 
   coord_cartesian(xlim = c(0,10.0)) +
    ggtitle("Veren HDL-kolestrolin normaali-jakauma")
  
#  ppc_dens_overlay(fshdl.true, fshdl.grep) + 
#  coord_cartesian(xlim = c(0,10.0)) +
#  ggtitle("Veren HDL-kolestrolin gamma-jakauma")

  ppc_dens_overlay(fshdl.true, posterior_g_50) + 
    coord_cartesian(xlim = c(0,10.0)) +
    ggtitle("Veren HDL-kolestrolin gamma-jakauma")

  ppc_dens_overlay(fsins.true, posterior_g_50) + 
    coord_cartesian(xlim = c(0,30.0)) +
    ggtitle("Veren insuliinin gamma-jakauma")

  ppc_dens_overlay(fpgluk.true, posterior_g_50) + 
    coord_cartesian(xlim = c(0,10.0)) +
    ggtitle("Veren glukoosin gamma-jakauma")

  ppc_dens_overlay(fsins.true, posterior_ln_50) + 
    coord_cartesian(xlim = c(0,50.0)) +
    ggtitle("Veren insuliinin lognormal-jakauma")
  
```



```{r}
#library("shinystan")
#gamma1 <- mebn.get_localfit("BLMM_gamma/ar1/fsins")
#launch_shinystan(gamma1)
```



```{r, echo=FALSE, eval=FALSE}
library(bayesplot)

fpgluk_normal <- mebn.get_localfit("BLMM_normal/fpgluk")
fpgluk_gamma <- mebn.get_localfit("BLMM_gamma/hierarchical_idlink/fpgluk")

fsins_normal <- mebn.get_localfit("BLMM_normal/fsins")
fsins_gamma_ar1 <- mebn.get_localfit("BLMM_gamma/ar1/fsins")

normal_plot <- plot(fpgluk_normal)
gamma_plot <- plot(fpgluk_gamma)

bayesplot_grid(plots = list(normal_plot, gamma_plot), legends = FALSE)

```

```{r}
# Compare intercepts

posterior_n <- extract(fsins_normal, pars = c("beta_Intercept", "beta", "sigma_e"))
posterior_g <- extract(fsins_gamma_ar1, pars = c("beta_Intercept", "beta", "sigma_e"))

mean(posterior_n$beta_Intercept) # 12
mean(posterior_g$beta_Intercept) # -9

#colMeans(posterior_n$beta)
#colMeans(posterior_g$beta)


```

## Pruning the graph

Fit of the local distributions is now reasonable well for considering the graphical model that it describes. For gaining better nutritional insight and improving the model generalization, we should still prune out insignificant nutrients and holding only most relevant as edges of the graph.

Vehtari and Piiroinen () provided a good overview of Bayesian model selection methods. First, we apply the Finnish horseshoe, a shrinkage prior, on regression coefficients. It allows specifying a prior knowledge, or at least an educated guess, about the number of significant predictors for a target. Here we guess that one third of the nutrients might be relevant for any given blood test, and apply following parameters for the shrinkage

```{r shrinkage_parameters, echo=TRUE, message=FALSE}
shrinkage_parameters <- within(list(),
{
    scale_icept  <- 1         # prior std for the intercept
    scale_global <- 0.01825   # scale for the half-t prior for tau: 
                              # ((p0=6) / (D=22-6)) * (sigma / sqrt(n=106*4))
    nu_global    <- 1         # degrees of freedom for the half-t priors for tau
    nu_local     <- 1         # degrees of freedom for the half-t priors for lambdas
    slab_scale   <- 1         # slab scale for the regularized horseshoe
    slab_df      <- 1         # slab degrees of freedom for the regularized horseshoe           
})
```

Next we fit the previous model with the Finnish horseshoe added

```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}

initial_graph <- mebn.new_graph_with_randomvariables(datadesc)
sysdimet_gamma_ar1_rhs <- mebn.bipartite_model(reaction_graph = initial_graph, 
                                   inputdata = sysdimet,
                                   predictor_columns = assumedpredictors, 
                                   assumed_targets = assumedtargets, 
                                   group_column = "SUBJECT_ID",
                                   local_estimation = mebn.sampling,
                                   local_model_cache = "models/BLMM_gamma/ar1_rhs", 
                                   stan_model_file = "mebn/BLMM_gamma_ar1_rhs.stan",
                                   reg_params = shrinkage_parameters,
                                   normalize_values = TRUE)
```

And then we test again how the shrinked model fits, and does its predictive score improve from non-shrinked model 

```{r normal_model_ppc, echo=FALSE, eval=TRUE}
mebn.target_dens_overlays("BLMM_gamma/ar1_rhs/", assumedtargets, sysdimet)
```

```{r, ar1_rhs_loo}
nonrhs_vs_rhs <- mebn.LOO_comparison(assumedtargets, "BLMM_gamma/ar1", "BLMM_gamma/ar1_rhs")
nonrhs_vs_rhs
```

Unfortunately, a visual comparison of PPCS show bias in model fits. This is further confirmed by a LOO comparison that shows little to significant drawbacks in prediction performance.

Thus, we discard the RHS model and proceed with previous full model with AR(1) structure. Projection method is shown to be a good for finding relevant model in the model space (viite). It seeks to find a minimal model that has 95% of predictive power of a full model.

The method has been implemented in projpred package 

```{r projpred}

## TODO: Repeat this for all local model in the graph

# Test with BLMM_gamma/ar1/fshdl 
target_column <- assumedtargets[assumedtargets$Name=="fshdl",]

# Get parameters that we used to fit the previous model
params <- mebn.set_model_parameters(assumedpredictors, target_column, group_column = "SUBJECT_ID", inputdata = sysdimet, normalize_values = TRUE)

# Initialize a reference model for variable selection

refm <- init_refmodel(z = params$X, 
                      y = params$Y, 
                      gaussian(), 
                      predfun = mebn.linpred, 
                      dis = sysdimet_gamma_ar1,
                      offset = NULL, 
                      intercept = TRUE,
                      cvfun = NULL, 
                      cvfits = NULL)




```
