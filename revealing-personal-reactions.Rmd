---
title: "Revealing the Personal Effects of Nutrition with Mixed-Effect Bayesian Network"
author:
- Jari Turkia, jari.turkia@cgi.com
- University of Eastern Finland
bibliography: biblio.bib
output:
  html_document: default
  pdf_document: default
abstract: This notebook describes the implementation of the modelling method and the experimental analysis that is described in the main article.
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE, fig.align="center")

# this allows using tikz rendering for plots with "dev=tikz"
knit_hooks$set(plot = function(x, options) {
  if ('tikz' %in% options$dev && !options$external) {
    hook_plot_tex(x, options)
  } else hook_plot_md(x, options)
})

# Load common MEBN package
source("mebn/MEBN.r")
```

In this work we propose a Bayesian network as an appealling way to model and predict the effects of nutrition. The Bayesian network is a directed graphical model where nodes of the graph are random variables and the edges between the nodes indicate an effect between variables. In the nutritional modelling we assign the amount of nutrients at the person's diet and the indicators of person's well-being those as random variables, and then we study the effects between them. The goal is then to find the connections for the graph that most probably describes the correct conditional relationships between nutrients and their effects.

In this article we use this method to analyze a dataset from the Sysdimet study [@pmid21901116] that contains repeated measurements of 17 nutrients, some basic information about patients (gender, medication) and blood test results. To ease the graph search, we focus only on two-level, bipartite, graphs where nutrients and personal details are assumed to affect blood tests, and only the magnitude of effect is left to be estimated. As we are interested in the personal variations of these nutritional effects we use a mixed-effect parametrization of Bayesian network [@Bae2016]. This allows us to estimate both typical and personal magnitudes of the nutritional effects with a same model.  

**Formal definition of the problem.** Let us denote the graph of interconnected nutrients and responses with $G$. We can then formulate the modeling problem as finding the graph $G$ that is the most probable given the data $D$

\begin{align}
{P}({G}|{D})
\end{align}

By using the Bayes' Rule we can be split this probability into proportions of the data likelihood of the given graph and any prior information we might have about suitable graphs 

\begin{align}
\label{prop_bayes_theorem}
{P}({G}|{D}) \propto {P}({D}|{G}) {P}({G})
\end{align}

This converts the problem in finding the graph with a maximal probability for the given data. To enforce our assumption of biologically plausible graphs, we set the graph prior \(P(G)\) to zero for all other graphs than the previously descibed bipartite graphs with nutrients affecting the bodily responses.

The joint probability of the graph \(P(G|D)\) can furthermore factorized into separate local probability distributions [@Bae2016, @Koller:2009:PGM:1795555] by their \textit{Markov blankets}. These probability distributions can be estimated separately and the novelity of our approach lies in their hierarchical mixed-effect estimation.

**Hierarchical estimation of the local distributions**. The data consists of repeated measurements from the patients. By modelling this 

##########
TODO: - esittele paikallisten jakaumien sekamallinnus ja miten jatkossa k채sitelt채v채t beta, b, b_sigma ym. parametrit liittyv채t verkkomalliin



Generally, the local probability distributions can be from exponential family of distributions, but in this case we consider only normally distributed response variables. The subset of parent variables, that we assume containing personal variance, is denoted with $pa_Z(X_i)$. For the mixed-effect modeling we need to estimate parameters \(\phi_i = \{\beta_i, b_i\}\) for expressing the typical and personal reaction types. In a multivariate normal model the uncertainty is furthermore defined by variance-covariance matrix $V_i$

\begin{align}
\begin{split}
\label{Normal LME}
{P}({X_i}|{pa(X_i), \phi_i, G_i}) = EXP-FAM({X_i} | pa(X_i)\beta_i + pa_Z(X_i)b_i, \sigma^2)
\end{split}
\end{align}

## Data

TODO: Esittele data verkkona.. alla..

The dataset in our experiment comes from Sysdimet study. ... variables, data points..

We have collected our prior information at a separate "Data description.csv"-file. For guiding the search for plausible graphs we have indicated which variables are responses at the graph and which are possible predictors affecting them.

```{r data_loading, echo=FALSE, message=FALSE}

# Read the data description
datadesc <- read.csv(file="Data description.csv", header = TRUE, sep = ";")

# Read the actual data matching the description
sysdimet <- read.csv(file="data/SYSDIMET_diet.csv", sep=";", dec=",")

# Define how to iterate through the graph
assumedpredictors <- datadesc[datadesc$Order==100,]    
assumedtargets <- datadesc[datadesc$Order==200,] 
```

**Constructing the graph**

Edges of the graph are absolute effects of nutrients.

```{r}
library(igraph)
initial_graph <- mebn.new_graph_with_randomvariables(datadesc)

V(initial_graph)$size = 5 
# - put all blood test values in own rank
bipa_layout <- layout_as_bipartite(initial_graph, types = V(initial_graph)$type == "100")
# - flip layout sideways, from left to right
gap <- 6
bipa_layout <- cbind(bipa_layout[,2]*gap, bipa_layout[,1])

plot(initial_graph,
       layout=bipa_layout, 
       rescale=TRUE,
       vertex.label.family="Helvetica",
       vertex.label.color="black",
       vertex.label.cex=1,
       vertex.label.dist=4,
       edge.arrow.size=0.5,
       edge.arrow.width=1)

```


We start from a hierarchical model with normal distribution that estimates of absolute effects are considered as reference when developing the model.

```{r graph_with_normal_rvs, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, cache=TRUE}
sysdimet_normal <- mebn.bipartite_model(reaction_graph = initial_normal_graph, 
                                         inputdata = sysdimet,
                                         predictor_columns = assumedpredictors, 
                                         assumed_targets = assumedtargets, 
                                         group_column = "SUBJECT_ID",
                                         local_estimation = mebn.sampling,
                                         local_model_cache = "models/BLMM_normal", 
                                         stan_model_file = "mebn/BLMM_normal.stan",
                                         normalize_values = TRUE)

```

**Normal model as reference**

If we now check these initial models of personal blood test responses, we can see that the Gaussian distribution is not a good fit as it allows negative blood test values and does not model the right tail of the true distribution. The estimated model parameters of Gaussian models are still kept as a reference point for further models. Those parameters are the weights of the edges at the Bayesian Network and it is important that all the random variables at the network can work with the same regression coefficients.

```{r normal_model_ppc, echo=FALSE, eval=TRUE, message=FALSE, cache=TRUE}
normal_targets <- assumedtargets
normal_targets$ScaleMin <- -10
normal_targets$ScaleMax <- 50

mebn.target_dens_overlays("BLMM_normal/", normal_targets, sysdimet)
```

**Developing the model**

- Biometric data is usually more multiplicative than additive. This is why log-normal distribution makes sense.
  Log-normal / Gamma

```{r graph_with_gamma_response, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, cache=TRUE}
initial_gamma_graph <- mebn.new_graph_with_randomvariables(datadesc)

sysdimet_gamma <- mebn.bipartite_model(reaction_graph = initial_gamma_graph, 
                                   inputdata = sysdimet,
                                   predictor_columns = assumedpredictors, 
                                   assumed_targets = assumedtargets, 
                                   group_column = "SUBJECT_ID",
                                   local_estimation = mebn.sampling,
                                   local_model_cache = "models/BLMM_gamma/hierarchical_idlink", 
                                   stan_model_file = "mebn/BLMM_gamma_hierarchical.stan",
                                   normalize_values = TRUE)
```

## Checking the fit of the local models

```{r gamma_ppc, echo=FALSE, eval=TRUE, message=FALSE, cache=TRUE}
mebn.target_dens_overlays("BLMM_gamma/hierarchical_idlink/", assumedtargets, sysdimet)
```

As we see here, the regression coefficients /(beta) are approximately the same for both Normal and Gamma models, but the confidence is better for the Gamma model 

```{r effect_comparison}
beta_coefs <- mebn.compare_typicals(sysdimet_normal, sysdimet_gamma)
head(beta_coefs)
```

**Comparing different local models with LOO**

Besides the visual inspection, we can also compare the models using LOO-PSIS information criteria. It uses log probability that is calculated as part of the Stan models.

```{r, normal_gamma_loo, message=FALSE, warning=FALSE, cache=TRUE}
normal_vs_gamma <- mebn.LOO_comparison(assumedtargets, "BLMM_normal", "BLMM_gamma/hierarchical_idlink")
normal_vs_gamma
# https://haozhu233.github.io/kableExtra/awesome_table_in_html.html
```

As the expected log predictive density (ELPD) of gamma model is higher, we should choose that for further development.

##Varying response time: AR-structures##

```{r graph_with_gamma_ar1, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, cache=TRUE}
initial_graph <- mebn.new_graph_with_randomvariables(datadesc)
sysdimet_gamma_ar1 <- mebn.bipartite_model(reaction_graph = initial_graph, 
                                   inputdata = sysdimet,
                                   predictor_columns = assumedpredictors, 
                                   assumed_targets = assumedtargets, 
                                   group_column = "SUBJECT_ID",
                                   local_estimation = mebn.sampling,
                                   local_model_cache = "models/BLMM_gamma/ar1", 
                                   stan_model_file = "mebn/BLMM_gamma_ar1.stan",
                                   normalize_values = TRUE)

mebn.write_gexf(sysdimet_gamma_ar1, "sysdimet_gamma_ar1.gexf")

# FSINS and FPLUK-models had undefined values in beta_Intercept and few in C-matrix. Maybe offset-value should be bigger?
```

```{r gamma_ar1_ppc, echo=FALSE, message=FALSE, warning=FALSE, eval=TRUE, cache=TRUE}
mebn.target_dens_overlays("BLMM_gamma/ar1/", assumedtargets, sysdimet)
```

```{r gamma_ar1_loo, message=FALSE, warning=FALSE, cache=TRUE}
ar0_vs_ar1 <- mebn.LOO_comparison(assumedtargets, "BLMM_gamma/hierarchical_idlink", "BLMM_gamma/ar1")
ar0_vs_ar1
```

```{r ar1_comparison, message=FALSE, warning=FALSE, cache=TRUE}
ar1_comparison <- mebn.AR_comparison(assumedtargets, "BLMM_gamma/ar1")
ar1_comparison
   
```

#Principal components as graph visualiation

Local distributions seem to have reasonable good fit. For better overall insight to nutritional effects we can next plot the Bayesian network that describes the connections between nutrients and their responses. (Beta and b nodes in PGM)

- Local distributions are estimated with Stan and values of the parameters are gathered as a Bayesian network
- This enables us to look at the joint probability as a whole
  - Principal components as a graph visualization 
  -- This shows the population effects

```{r, typical_effects_figure, fig.height = 8, fig.width = 10, fig.align = "center", message=FALSE, warning=FALSE, cache=TRUE}
mebn.plot_typical_effects(sysdimet_gamma_ar1, 15)
```

#Probabilistic graphical model

Our MEBN functions extract a propabilistic graphical model from local distributions estimated with Stan. We can us this PGM to analysis. 

- We can query the graph for all the sigma_b-nodes that represent variation of the effect between persons
- Effects are sorted by magnitude and confidence

```{r, echo=TRUE, eval=TRUE}
#sysdimet_gamma_ar1 <- mebn.read_gexf("sysdimet_gamma_ar1.gexf") ei lue oikein?

# Query the graph for personal variances, denoted by b_sigma-nodes
allnodes <- V(sysdimet_gamma_ar1)
b_sigma <- allnodes[allnodes$type=="b_sigma"]
#beta_nodes <- allnodes[allnodes$type=="beta"]

# Again, a separate data frame is constructed for printing
personal_variances<-data.frame(matrix(NA, nrow=length(b_sigma), ncol=0))

personal_variances$effect <- unlist(lapply(strsplit(gsub("b_sigma_","", b_sigma$name), "_"), function(x) paste0(toString(datadesc[datadesc$Name==x[1],]$Description)," -> ", toString(datadesc[datadesc$Name==x[2],]$Description))))

personal_variances$variance <- b_sigma$value
personal_variances$"CI-10%" <- b_sigma$value_lCI
personal_variances$"CI-90%" <- b_sigma$value_uCI

ordered_personal_variance <- personal_variances[order(-personal_variances$variance),]

head(ordered_personal_variance, 40)
```

Let us examine closer those of the effects that have most probable variance over 0.10

```{r number_of_varying_effects}
effects_with_most_variance <- ordered_personal_variance[ordered_personal_variance$variance >= 0.30,]
number_of_varying_effects <- nrow(effects_with_most_variance)
```

For better undestanding, we can also visualize these personal variances as a graph

```{r personal_variations_figure, fig.height = 7, fig.width = 10, fig.align = "center", cache = TRUE}
source("mebn/MEBN.r")
mebn.plot_personal_variations(sysdimet_gamma_ar1, number_of_varying_effects)
```

Although there are personal differences, it is likely that not everyone behave uniquely but there might exist similar groups of behavior. We can analyze personal differences in two ways. We can look the absolute magnitudes of effects and we can also look how persons differ from typical behavior or mean of the effect.

We can now take the personal estimations of the effects and see, if they form clusters

```{r kmeans_dfs, echo=FALSE, cache=FALSE}
library(rstan)

# Pick these predictors as features for clustering -- get all
feature_index <- c(1:nrow(assumedpredictors))

personal_variations_from_mean <- matrix(NA, ncol=length(feature_index)*nrow(assumedtargets), nrow=length(levels(sysdimet$SUBJECT_ID)))
personal_effects <- matrix(NA, ncol=length(feature_index)*nrow(assumedtargets), nrow=length(levels(sysdimet$SUBJECT_ID)))

modelcache <- "BLMM_gamma/ar1/"

# We could fetch a personal graph for all patients, but it is more effective to extract estimations directly from MCMC-samples to data frames

for (i in c(1:nrow(assumedtargets)))
{
  targetname <- as.vector(assumedtargets[i,]$Name)
  
  target_blmm <- mebn.get_localfit(paste0(modelcache,targetname))
  posterior <- extract(target_blmm, pars = c("personal_effect", "b"))

  b_blmm <- colMeans(posterior$b)
  beta_b_blmm <- colMeans(posterior$personal_effect)
  
  # Omit predictor columns here, if needed
  # - b has intercept as first column, so the index needs to be adjusted
  b_blmm <- b_blmm[,feature_index+1] 
  beta_b_blmm <- beta_b_blmm[,feature_index] 
  
  if (i == 1) {
    # variance of the intercept is omitted
    personal_variations_from_mean <- b_blmm
    personal_effects <- beta_b_blmm
  }
  else
  {
    personal_variations_from_mean <- cbind(personal_variations_from_mean, b_blmm)
    personal_effects <- cbind(personal_effects, beta_b_blmm)
  }
}

```

```{r personal_effects, cache=FALSE}

# Pull effects form all patients to one vector
nperson <- 106
largest_personal_effects <- as.data.frame(as.vector(personal_effects))
colnames(largest_personal_effects) <- c("amount")
largest_personal_effects$abs_amount <- abs(largest_personal_effects$amount)
largest_personal_effects$predictor <- rep(rep(assumedpredictors[feature_index,]$Description,nrow(assumedtargets)), nperson)

cl <- rep(1,length(feature_index)) # number of predictors
t_idx <- c(cl,cl+1,cl+2,cl+3,cl+4) # predictors x targets
largest_personal_effects$response <- rep(assumedtargets$Description[t_idx], nperson)

largest_personal_effects$effect <- paste0(largest_personal_effects$predictor," -> ", largest_personal_effects$response)

largest_personal_effects <- largest_personal_effects[largest_personal_effects$abs_amount > 2,]
largest_personal_effects <- largest_personal_effects[order(-largest_personal_effects$abs_amount),]
largest_personal_effects <- largest_personal_effects[c("effect", "amount")]

```

## Latent grouping

```{r, echo=FALSE, eval=TRUE}
library(stats)
library(gridExtra)

k.max <- 8
wss_effects <- sapply(1:k.max, function(k){kmeans(personal_effects, k, nstart=50, iter.max = 15)$tot.withinss})
wss_vars <- sapply(1:k.max, function(k){kmeans(personal_variations_from_mean, k, nstart=50, iter.max = 15)$tot.withinss})

df_vars <- data.frame(x = 1:k.max, y = wss_vars)
df_effects <- data.frame(x = 1:k.max, y = wss_effects)

plot1 <- ggplot(data=df_effects, aes(x=x, y=y, group=1)) +
  geom_line() +
  geom_point() + 
  ggtitle("Difference in absolute reaction") +
  xlab("Number of clusters") +
  ylab("Difference between clusters")

plot2 <- ggplot(data=df_vars, aes(x=x, y=y, group=1)) +
  geom_line() +
  geom_point() + 
  ggtitle("Difference from typical behavior") +
  xlab("Number of clusters") +
  ylab("Difference between clusters")

gridExtra::grid.arrange(plot1,plot2,nrow=1)
```

### Difference from typical behavior
  
By looking previous diagram we see that there are four clearly identifiable groups between patients. At this plot, zero of X-axis denotes a typical behaviour of that particular reaction, and blue and red bars denote a deviation from this typical mean.

```{r variation clusters1, echo=FALSE, eval=TRUE, fig.width=7, fig.height=10}
source("mebn/MEBN.r")

# Number of cluster centers 
k <- 4

# Clustering using k-means
km <- kmeans(personal_variations_from_mean, centers = k)

# Every patient can be assigned to some cluster based on how their reactions differ from average
sysdimet$variation_group <- km$cluster 

# Plot the clusters
variations_data <- as.data.frame(t(km$centers))
mebn.plot_clusters(variations_data, assumedpredictors, assumedtargets, effects_with_most_variance, feature_index, sort_by_amount = TRUE)
```

Cholesterol medication seems to dominate the clusters

Let's see how the clustering shows with absolute effects rather than difference from mean

```{r variation clusters2, echo=FALSE, eval=TRUE, fig.width=7, fig.height=15}
source("mebn/MEBN.r")

# Number of cluster centers 
k <- 4

# Clustering using k-means
km <- kmeans(personal_effects, centers = k)

# Every patient can be assigned to some cluster also based on how their personal reactions types

# - repeat same cluster placement for all the observations of same patient
observations_per_patient <- 4
sysdimet$effect_group <- unlist(lapply(km$cluster,rep,observations_per_patient)) 

# Plot the clusters
variations_data <- as.data.frame(t(km$centers))
mebn.plot_clusters(variations_data, assumedpredictors, assumedtargets, largest_personal_effects, feature_index, sort_by_amount = FALSE)

```

The rising effect of cholesterol medication to blood insulin seems to be dominating these clusters. Let's create a cross tabulation to explore this effect more closely.

```{r cholmed_crosstab}

table(sysdimet$kolestrolilaakitys, sysdimet$effect_group)

```

Persons in clusters 1 and 4 are taking cholesterol medications, and for them it seems to rise cholesterol levels.

TODO: Plot insulin levels of groups?

Both gender and cholesterol medication are unchanging factors at the dataset. Let us next remove those from clustering features and see how the clusters form based on nutritional effects only.

```{r}
feature_index <- feature_index[-c(match("sukupuoli", assumedpredictors$Name), 
                                  match("kolestrolilaakitys", assumedpredictors$Name))]


personal_variations_from_mean <- matrix(NA, ncol=length(feature_index)*nrow(assumedtargets), nrow=length(levels(sysdimet$SUBJECT_ID)))
personal_effects <- matrix(NA, ncol=length(feature_index)*nrow(assumedtargets), nrow=length(levels(sysdimet$SUBJECT_ID)))

modelcache <- "BLMM_gamma/ar1/"

for (i in c(1:nrow(assumedtargets)))
{
  targetname <- as.vector(assumedtargets[i,]$Name)
  
  target_blmm <- mebn.get_localfit(paste0(modelcache,targetname))
  posterior <- extract(target_blmm, pars = c("personal_effect", "b"))

  b_blmm <- colMeans(posterior$b)
  beta_b_blmm <- colMeans(posterior$personal_effect)
  
  # Omit predictor columns here, if needed
  # - b has intercept as first column, so the index needs to be adjusted
  b_blmm <- b_blmm[,feature_index+1] 
  beta_b_blmm <- beta_b_blmm[,feature_index] 
  
  if (i == 1) {
    # variance of the intercept is omitted
    personal_variations_from_mean <- b_blmm
    personal_effects <- beta_b_blmm
  }
  else
  {
    personal_variations_from_mean <- cbind(personal_variations_from_mean, b_blmm)
    personal_effects <- cbind(personal_effects, beta_b_blmm)
  }
}
```

Number of clusters with nutritional effects only

```{r, echo=FALSE, eval=FALSE}
library(stats)
library(gridExtra)

k.max <- 8
wss_effects <- sapply(1:k.max, function(k){kmeans(personal_effects, k, nstart=50, iter.max = 15)$tot.withinss})
wss_vars <- sapply(1:k.max, function(k){kmeans(personal_variations_from_mean, k, nstart=50, iter.max = 15)$tot.withinss})

df_vars <- data.frame(x = 1:k.max, y = wss_vars)
df_effects <- data.frame(x = 1:k.max, y = wss_effects)

plot1 <- ggplot(data=df_effects, aes(x=x, y=y, group=1)) +
  geom_line() +
  geom_point() + 
  ggtitle("Difference in absolute reaction") +
  xlab("Number of clusters") +
  ylab("Difference between clusters")

plot2 <- ggplot(data=df_vars, aes(x=x, y=y, group=1)) +
  geom_line() +
  geom_point() + 
  ggtitle("Difference from typical behavior") +
  xlab("Number of clusters") +
  ylab("Difference between clusters")

gridExtra::grid.arrange(plot1,plot2,nrow=1)
```

```{r variation clusters3, echo=FALSE, eval=TRUE, fig.width=7, fig.height=10}
source("mebn/MEBN.r")
# Number of cluster centers 
k <- 4

# Clustering using k-means
km <- kmeans(personal_variations_from_mean, centers = k)

# Every patient can be assigned to some cluster based on how their reactions differ from average
sysdimet$variation_group <- km$cluster 

# Plot the clusters
variations_data <- as.data.frame(t(km$centers))
mebn.plot_clusters(variations_data, assumedpredictors, assumedtargets, effects_with_most_variance, feature_index, sort_by_amount = TRUE)
```

```{r variation clusters, echo=FALSE, eval=TRUE, fig.width=7, fig.height=10}

# Number of cluster centers 
k <- 4

# Clustering using k-means
km <- kmeans(personal_effects, centers = k)

# Every patient can be assigned to some cluster also based on how their personal reactions types

# - repeat same cluster placement for all the observations of same patient
observations_per_patient <- 4
sysdimet$effect_group <- unlist(lapply(km$cluster,rep,observations_per_patient)) 

# Plot the clusters
variations_data <- as.data.frame(t(km$centers))
mebn.plot_clusters(variations_data, assumedpredictors, assumedtargets, effects_with_most_variance, feature_index)
```


### Predicting personal reaction type

Finally, we can pick one the previously found reaction type clusters and hold aside the data for few of these patients. The goal would be then to use our model to predict the blood test values for this test group by their diet. This also reveals their personal reaction type for nutrition. We can compare this prediction to the true blood test values at dataset and also to the previous estimates from the whole sample population.

```{r holdout_data}
# TODO: ota esim. vain 10 t채st채 klusterista, jotta mallissa on mukana my철s t채m채n klusterin potilaita 
holdout_data <- sysdimet[sysdimet$variation_group==3,]
training_data <- sysdimet[!(sysdimet$SUBJECT_ID %in% holdout_data$SUBJECT_ID),]
```

The predictive ability of the model might be reduced by overfitting to small nuances of the whole dataset. To overcome the possible overfitting, we apply the Finnish horseshoe, a shrinkage prior, on regression coefficients. It allows specifying a prior knowledge, or at least an educated guess, about the number of significant predictors for a target. Here we guess that one third of the nutrients might be relevant for any given blood test, and apply following parameters for the shrinkage

```{r shrinkage_parameters, echo=TRUE, message=FALSE}
shrinkage_parameters <- within(list(),
{
    scale_icept  <- 1         # prior std for the intercept
    scale_global <- 0.01825   # scale for the half-t prior for tau: 
                              # ((p0=6) / (D=22-6)) * (sigma / sqrt(n=106*4))
    nu_global    <- 1         # degrees of freedom for the half-t priors for tau
    nu_local     <- 1         # degrees of freedom for the half-t priors for lambdas
    slab_scale   <- 1         # slab scale for the regularized horseshoe
    slab_df      <- 1         # slab degrees of freedom for the regularized horseshoe           
})
```

Next we fit the previous model with the Finnish horseshoe added. Besides the shrinkage parameters, we provide now the data in two sets: input data for estimation and target data to hold out for prediction. 

```{r, echo=TRUE, eval=FALSE, message=FALSE, warning=FALSE}

sysdimet_gamma_ar1_rhs_pred <- mebn.bipartite_model(reaction_graph = initial_graph, 
                                   inputdata = training_data,
                                   targetdata = holdout_data,
                                   predictor_columns = assumedpredictors, 
                                   assumed_targets = assumedtargets, 
                                   group_column = "SUBJECT_ID",
                                   local_estimation = mebn.sampling,
                                   local_model_cache = paste0("models/BLMM_gamma/ar1_rhs_pred"), 
                                   stan_model_file = "mebn/BLMM_gamma_ar1_rhs_pred.stan",
                                   reg_params = shrinkage_parameters,
                                   normalize_values = TRUE)
```

```{r, eval=FALSE}
#remove("models_BLMM_gamma_ar1_rhs_pred_1_fshdl_blmm")
source("mebn/MEBN.r")
library(rstan)
library(bayesplot)
  
fit1  <- mebn.get_localfit("BLMM_gamma/ar1_rhs_pred/36/fshdl")

fshdl.true <- holdout_data$fshdl
pred_post <- extract(fit1, pars = c("Y_pred"))
post_50 <- na.omit(pred_post$Y_pred[seq(2000,4000,10),])

fshdl.true
colMeans(na.omit(pred_post$Y_pred))

# Can we predict how this person reacts? What nutrients increase or decrease values on average?

#ppc_dens_overlay(fshdl.true, post_50) + 
#coord_cartesian(xlim = c(0,5.0)) +
#ggtitle("fshdl S36")

```

Bayes R2

```{r, eval=FALSE}

# Andrew Gelman, Ben Goodrich, Jonah Gabry, and Aki Vehtari (2018). R-squared for Bayesian regression models. The American Statistician
# https://doi.org/10.1080/00031305.2018.1549100
# https://avehtari.github.io/bayes_R2/bayes_R2.html (supplement notebook)

bayes_R2 <- function(y, ypred) {
  e <- -1 * sweep(ypred, 2, y)
  var_ypred <- apply(ypred, 1, var)
  var_e <- apply(e, 1, var)
  var_ypred / (var_ypred + var_e)
}

R2 <- bayes_R2(fshdl.true, na.omit(pred_post$Y_pred))
print(median(R2))
```

