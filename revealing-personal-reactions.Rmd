---
title: "Revealing the Personal Effects of Nutrition with Mixed-Effect Bayesian Network"
author:
- Jari Turkia, jari.turkia@cgi.com
- University of Eastern Finland
bibliography: biblio.bib
output:
  html_document: default
  pdf_document: default
abstract: This notebook describes the implementation of the modelling method and the experimental analysis that is described in the main article.
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE, fig.align="center")

# this allows using tikz rendering for plots with "dev=tikz"
knit_hooks$set(plot = function(x, options) {
  if ('tikz' %in% options$dev && !options$external) {
    hook_plot_tex(x, options)
  } else hook_plot_md(x, options)
})

# Fix seed for random number generator for getting consistent results in kmeans etc.
fixed_seed <- 678

# Load common MEBN package
source("mebn/MEBN.r")
```

In this work we propose a Bayesian network as an appealling way to model and predict the effects of nutrition. The Bayesian network is a directed graphical model where nodes of the graph are random variables and the edges between the nodes indicate an effect between variables. In the nutritional modelling we assign the amount of nutrients at the person's diet and the indicators of person's well-being those as random variables, and then we study the effects between them. The goal is then to find the connections for the graph that most probably describes the correct conditional relationships between nutrients and their effects.

In this article we use this method to analyze a dataset from the Sysdimet study [@pmid21901116] that contains repeated measurements of 17 nutrients, some basic information about patients (gender, medication) and blood test results. To ease the graph search, we focus only on two-level, bipartite, graphs where nutrients and personal details are assumed to affect blood tests, and only the magnitude of effect is left to be estimated. As we are interested in the personal variations of these nutritional effects we use a mixed-effect parametrization of Bayesian network [@Bae2016]. This allows us to estimate both typical and personal magnitudes of the nutritional effects with a same model.  

**Formal definition of the problem.** Let us denote the graph of interconnected nutrients and responses with $G$. We can then formulate the modeling problem as finding the graph $G$ that is the most probable given the data $D$

\begin{align}
{P}({G}|{D})
\end{align}

By using the Bayes' Rule we can be split this probability into proportions of the data likelihood of the given graph and any prior information we might have about suitable graphs 

\begin{align}
\label{prop_bayes_theorem}
{P}({G}|{D}) \propto {P}({D}|{G}) {P}({G})
\end{align}

This turns the problem to finding a graph that is most probable given data. To enforce our assumption of biologically plausible graphs, we set the graph prior \(P(G)\) to zero for all other graphs than the previously descibed bipartite graphs with nutrients affecting the bodily responses.

The joint probability of the graph \(P(G|D)\) can furthermore factorized into separate local probability distributions [@Bae2016, @Koller:2009:PGM:1795555] by their \textit{Markov blankets}. These probability distributions can be estimated separately and the novelity of our approach lies in their hierarchical mixed-effect estimation.

**Hierarchical estimation of the local distributions**. We are interested in the personal variations of how different nutrients affect. For this we need repeated measurements of nutrients in person's diet and the corresponding bodily responses. By collecting these personal sets of measurements we obtain a dataset from where we can estimate both typical and personal effects of nutrition. Modeling correctly this hierarchical structure of data should give us the best fitting model and also the parameters that explain the magnitudes of effects.

Concretely, we can use generalized linear mixed-effect models to describe the local probability distributions of the Bayesian network. We assume that every  random variable at the network can be explaned with a probability distribution from an exponential family of distributions with linear link function with input from its parent nodes. We furthermore assume that the effect between variables can be factorized into typical and personal parts. Here typical part of the effect is denoted with coefficient \beta_i and personal part as \b_i

\begin{align}
\begin{split}
\label{EXPFAM LME}
{P}({Y_i}|{pa(Y_i), \phi_i, G_i}) = {EXPFAM}({Y_i} | pa(X_i)\beta_i + pa_Z(X_i)b_i, \sigma^2)
\end{split}
\end{align}

Note that while this formulation generalizes to graphs with multiple levels, we have a prior assumption ({P}({G})) that the structure of our dataset is a bipartite graph. 

```{r data_loading, echo=FALSE, message=FALSE}

# Read the data description
datadesc <- read.csv(file="Data description.csv", header = TRUE, sep = ";")

# Read the actual data matching the description
sysdimet <- read.csv(file="data/SYSDIMET_diet.csv", sep=";", dec=",")

# Define how to iterate through the graph
assumedpredictors <- datadesc[datadesc$Order==100,]    
assumedtargets <- datadesc[datadesc$Order==200,] 
```

```{r graph, fig.height = 8, fig.width = 10, fig.align = "center"}
library(igraph)
initial_graph <- mebn.fully_connected_bipartite_graph(datadesc)

V(initial_graph)$size = 10 
# - put all blood test values in own rank
bipa_layout <- layout_as_bipartite(initial_graph, types = V(initial_graph)$type == "100")
# - flip layout sideways, from left to right
gap <- 6
bipa_layout <- cbind(bipa_layout[,2]*gap, bipa_layout[,1])

V(initial_graph)[V(initial_graph)$type == "100"]$label.degree = pi # left side
V(initial_graph)[V(initial_graph)$type == "200"]$label.degree = 0 # right side

plot(initial_graph,
       layout=bipa_layout, 
       rescale=TRUE,
       vertex.label.family="Helvetica",
       vertex.label.color="black",
       vertex.label.cex=1,
       vertex.label.dist=4,
       edge.arrow.size=0.5,
       edge.arrow.width=1)

```

We assume a connection from every nutrient and personal information to every blood test value at the dataset. The model search is then focused on to estimating optimal coefficients \beta and \b_i that we use to define the strength of the connection. It is also important to find correct probability distributions for the random variables.

Our starting assumption is that both nutrients and the blood test values are normally distributed. This assumption is naive as it allows the blood tests to have negative values. We normalize all the input values to same scale, so that the estimated regression coefficients can be used as a indicators of connection strenght.

```{r graph_with_normal_rvs, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, cache=TRUE}
sysdimet_normal <- mebn.bipartite_model(reaction_graph = initial_graph, 
                                         inputdata = sysdimet,
                                         predictor_columns = assumedpredictors, 
                                         assumed_targets = assumedtargets, 
                                         group_column = "SUBJECT_ID",
                                         local_estimation = mebn.sampling,
                                         local_model_cache = "models/BLMM_normal", 
                                         stan_model_file = "mebn/BLMM_normal.stan",
                                         normalize_values = TRUE)

```

We can assess the model fit by plotting the posterior predictive distributions (PPC) over the distributions of the true blood test values. As expected, the Gaussian random variables are not fitting well.

```{r normal_model_ppc, fig.height = 4, fig.width = 6, fig.align = "center", echo=FALSE, eval=TRUE, message=FALSE, cache=TRUE}
normal_targets <- assumedtargets[assumedtargets$Name=="fshdl",]
normal_targets$ScaleMin <- -10
normal_targets$ScaleMax <- 25

mebn.target_dens_overlays("BLMM_normal/", normal_targets, sysdimet)
```

**Developing the model**

- Biometric data is usually more multiplicative than additive. This is why log-normal distribution makes sense.
  Log-normal / Gamma
  
  https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0021403
  Citation: Limpert E, Stahel WA (2011) Problems with Using the Normal Distribution – and Ways to Improve Quality and Efficiency of Data Analysis. PLoS ONE 6(7): e21403. https://doi.org/10.1371/journal.pone.0021403
  
- We use identity link as it keeps the regression coefficients on the same absolute scale than with Gaussian models

\begin{align}
\begin{split}
\label{Gamma LME}
{P}({Y_i}|{pa(Y_i), \phi_i, G_i}) = Gamma({Y_i} | \alpha, \alpha / pa(X_i)\beta_i + pa_Z(X_i)b_i)
\end{split}
\end{align}

```{r graph_with_gamma_response, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, cache=TRUE}
initial_gamma_graph <- mebn.new_graph_with_randomvariables(datadesc)

sysdimet_gamma <- mebn.bipartite_model(reaction_graph = initial_gamma_graph, 
                                   inputdata = sysdimet,
                                   predictor_columns = assumedpredictors, 
                                   assumed_targets = assumedtargets, 
                                   group_column = "SUBJECT_ID",
                                   local_estimation = mebn.sampling,
                                   local_model_cache = "models/BLMM_gamma/hierarchical_idlink", 
                                   stan_model_file = "mebn/BLMM_gamma_hierarchical.stan",
                                   normalize_values = TRUE)
```

## Checking the fit of the local models

```{r gamma_ppc, echo=FALSE, eval=TRUE, message=FALSE, cache=TRUE}
mebn.target_dens_overlays("BLMM_gamma/hierarchical_idlink/", assumedtargets, sysdimet)
```

As we see here, the regression coefficients {\beta} are approximately the same for both Normal and Gamma models, but the confidence is better for the Gamma model 

```{r effect_comparison}
beta_coefs <- mebn.compare_typicals(sysdimet_normal, sysdimet_gamma)
head(beta_coefs)
```

**Comparing different local models with LOO**

Besides the visual inspection, we can also compare the models using LOO-PSIS information criteria. It uses log probability that is calculated as part of the Stan models.

```{r, normal_gamma_loo, message=FALSE, warning=FALSE, cache=TRUE}
normal_vs_gamma <- mebn.LOO_comparison(assumedtargets, "BLMM_normal", "BLMM_gamma/hierarchical_idlink")
normal_vs_gamma
# https://haozhu233.github.io/kableExtra/awesome_table_in_html.html
```

( Is P(D|G_i) = SUM(ELPD_i)? )

As the expected log predictive density (ELPD) of gamma model is higher, we should choose that for further development.

##Varying response time: AR-structures##

```
...
    // - calculate residuals     
    e[n] = Y[n] - mu[n]; 

    // - estimate autoregression coefficient for observations of one patient 
    if (group_size > 1)
      mu[n] += e[n-1] * ar1;
      
...      
```


```{r graph_with_gamma_ar1, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE, cache=TRUE}
initial_graph <- mebn.new_graph_with_randomvariables(datadesc)
sysdimet_gamma_ar1 <- mebn.bipartite_model(reaction_graph = initial_graph, 
                                   inputdata = sysdimet,
                                   predictor_columns = assumedpredictors, 
                                   assumed_targets = assumedtargets, 
                                   group_column = "SUBJECT_ID",
                                   local_estimation = mebn.sampling,
                                   local_model_cache = "models/BLMM_gamma/ar1", 
                                   stan_model_file = "mebn/BLMM_gamma_ar1.stan",
                                   normalize_values = TRUE)

mebn.write_gexf(sysdimet_gamma_ar1, "sysdimet_gamma_ar1.gexf")
write_graph(sysdimet_gamma_ar1, "sysdimet_gamma_ar1.graphml", "graphml")

```

```{r gamma_ar1_ppc, echo=FALSE, message=FALSE, warning=FALSE, eval=TRUE, cache=TRUE}
mebn.target_dens_overlays("BLMM_gamma/ar1/", assumedtargets, sysdimet)
```

```{r gamma_ar1_loo, message=FALSE, warning=FALSE, cache=TRUE}
ar0_vs_ar1 <- mebn.LOO_comparison(assumedtargets, "BLMM_gamma/hierarchical_idlink", "BLMM_gamma/ar1")
ar0_vs_ar1
```

```{r ar1_comparison, message=FALSE, warning=FALSE, cache=TRUE}
ar1_comparison <- mebn.AR_comparison(assumedtargets, "BLMM_gamma/ar1")
ar1_comparison
   
```

#Typical effects of nutrition

We have now reached a reasonable good fit for the local distributions of the Bayesian network. Our code has extracted a graphical model from these full posterior distributions of random variables and parameters. The graph consists mean values of the posteriors and also their credible intervals. This is a lighter data structure for further analysis and allows also graph operations besides the regression modeling. 

Besides the random variables, the graph also includes the regression coefficients \beta and \b that denote the typical and personal magnitudes of effects for different nutrients. For better overview, let us plot the graph of typical nutritional effects. This visualization shows typical coefficients \beta as weight of the connection between blood test and nutrients at diet affecting it. For clarity, the visualization only shows 15 principal components explaining the variance of each blood test.

```{r, typical_effects_figure, fig.height = 8, fig.width = 10, fig.align = "center", message=FALSE, warning=FALSE, cache=TRUE}
mebn.plot_typical_effects(sysdimet_gamma_ar1, 15)
```

#Personal effects

Beside the typical effects, our main interest lies on the personal effects \b and their variances {\b_sigma}. If there is a large variance for the personal effect it means that there exists personal differences in reaction types. These differences are our main interest to find.

In this dataset there exist 20*5=100 possible effects between nutrients and blood tests. For finding the most interesting effects with most personal variance, we extract and sort b_sigma-hyperparameters from the graph.

```{r, echo=FALSE, message=FALSE, eval=TRUE}
#sysdimet_gamma_ar10 <- mebn.read_gexf("sysdimet_gamma_ar1.gexf") 
sysdimet_gamma_ar1 <- read_graph("sysdimet_gamma_ar1.graphml", "graphml")

# Query the graph for personal variances, denoted by b_sigma-nodes
allnodes <- V(sysdimet_gamma_ar1)
b_sigma <- allnodes[allnodes$type=="b_sigma"]
#beta_nodes <- allnodes[allnodes$type=="beta"]

# Again, a separate data frame is constructed for printing
personal_variances<-data.frame(matrix(NA, nrow=length(b_sigma), ncol=0))

personal_variances$effect <- unlist(lapply(strsplit(gsub("b_sigma_","", b_sigma$name), "_"), function(x) paste0(toString(datadesc[datadesc$Name==x[1],]$Description)," -> ", toString(datadesc[datadesc$Name==x[2],]$Description))))

personal_variances$variance <- b_sigma$value
personal_variances$"CI-10%" <- b_sigma$value_lCI
personal_variances$"CI-90%" <- b_sigma$value_uCI

ordered_personal_variance <- personal_variances[order(-personal_variances$variance),]

head(ordered_personal_variance, 20)
```

Let us the examine closer those of the effects that have most probable variance over 0.30

```{r number_of_varying_effects}
effects_with_most_variance <- ordered_personal_variance[ordered_personal_variance$variance >= 0.30,]
number_of_varying_effects <- nrow(effects_with_most_variance)
```

For better undestanding, we can also visualize these personal variances again as a graph

```{r personal_variations_figure, fig.height = 8, fig.width = 10, fig.align = "center", cache = TRUE}
mebn.plot_personal_variations(sysdimet_gamma_ar1, number_of_varying_effects)
```

TODO: plot varcov-matrix Sigma_b (ggplot2 heatmap)

##Clusters of personal graphs

Although there are personal differences, it is likely that not everyone behave uniquely but there might exist similar groups of behavior. We can analyze personal differences in two ways. We can look the absolute magnitudes of effects and we can also look how persons differ from typical behavior or mean of the effect.

We can now take the personal estimations of the effects and see, if they form clusters

```{r kmeans_dfs, echo=FALSE, cache=FALSE, message=FALSE}
library(rstan)

# Pick these predictors as features for clustering -- get all
feature_index <- c(1:nrow(assumedpredictors))

personal_variations_from_mean <- matrix(NA, ncol=length(feature_index)*nrow(assumedtargets), nrow=length(levels(sysdimet$SUBJECT_ID)))
personal_effects <- matrix(NA, ncol=length(feature_index)*nrow(assumedtargets), nrow=length(levels(sysdimet$SUBJECT_ID)))

modelcache <- "BLMM_gamma/ar1/"

# We could fetch a personal graph for all patients, but it is more effective to extract estimations directly from MCMC-samples to data frames
# TODO: use mebn.personal_effects. Do we need CIs here?


for (i in c(1:nrow(assumedtargets)))
{
  targetname <- as.vector(assumedtargets[i,]$Name)
  
  target_blmm <- mebn.get_localfit(paste0(modelcache,targetname))
  posterior <- extract(target_blmm, pars = c("personal_effect", "b"))

  b_blmm <- colMeans(posterior$b)
  beta_b_blmm <- colMeans(posterior$personal_effect)
  
  # Omit predictor columns here, if needed
  # - b has intercept as first column, so the index needs to be adjusted
  b_blmm <- b_blmm[,feature_index+1] 
  beta_b_blmm <- beta_b_blmm[,feature_index] 
  
  if (i == 1) {
    # variance of the intercept is omitted
    personal_variations_from_mean <- b_blmm
    personal_effects <- beta_b_blmm
  }
  else
  {
    personal_variations_from_mean <- cbind(personal_variations_from_mean, b_blmm)
    personal_effects <- cbind(personal_effects, beta_b_blmm)
  }
}

```

```{r personal_effects, echo=FALSE, cache=FALSE}

# Pull effects form all patients to one vector
nperson <- 106
largest_personal_effects <- as.data.frame(as.vector(personal_effects))
colnames(largest_personal_effects) <- c("amount")
largest_personal_effects$abs_amount <- abs(largest_personal_effects$amount)
largest_personal_effects$predictor <- rep(rep(assumedpredictors[feature_index,]$Description,nrow(assumedtargets)), nperson)

cl <- rep(1,length(feature_index)) # number of predictors
t_idx <- c(cl,cl+1,cl+2,cl+3,cl+4) # predictors x targets
largest_personal_effects$response <- rep(assumedtargets$Description[t_idx], nperson)

largest_personal_effects$effect <- paste0(largest_personal_effects$predictor," -> ", largest_personal_effects$response)

largest_personal_effects <- largest_personal_effects[largest_personal_effects$abs_amount > 2,]
largest_personal_effects <- largest_personal_effects[order(-largest_personal_effects$abs_amount),]
largest_personal_effects <- largest_personal_effects[c("effect", "amount")]

```

## Latent grouping

```{r, echo=FALSE, eval=TRUE}
library(stats)
library(gridExtra)

set.seed(fixed_seed)
k.max <- 8
wss_effects <- sapply(1:k.max, function(k){kmeans(personal_effects, k, nstart=50, iter.max = 15)$tot.withinss})
wss_vars <- sapply(1:k.max, function(k){kmeans(personal_variations_from_mean, k, nstart=50, iter.max = 15)$tot.withinss})

df_vars <- data.frame(x = 1:k.max, y = wss_vars)
df_effects <- data.frame(x = 1:k.max, y = wss_effects)

plot1 <- ggplot(data=df_effects, aes(x=x, y=y, group=1)) +
  geom_line() +
  geom_point() + 
  ggtitle("Difference in absolute reaction") +
  xlab("Number of clusters") +
  ylab("Difference between clusters")

plot2 <- ggplot(data=df_vars, aes(x=x, y=y, group=1)) +
  geom_line() +
  geom_point() + 
  ggtitle("Difference from typical behavior") +
  xlab("Number of clusters") +
  ylab("Difference between clusters")

gridExtra::grid.arrange(plot1,plot2,nrow=1)
```

### Difference from typical behavior
  
By looking previous diagram we see that there are four clearly identifiable groups between patients. At this plot, zero of X-axis denotes a typical behaviour of that particular reaction, and blue and red bars denote a deviation from this typical mean.

```{r variation clusters1, echo=FALSE, eval=TRUE, fig.width=7, fig.height=10}
source("mebn/MEBN.r")

# Number of cluster centers 
k <- 4

# Clustering using k-means
set.seed(fixed_seed)
km <- kmeans(personal_variations_from_mean, centers = k)

# Every patient can be assigned to some cluster based on how their reactions differ from average
sysdimet$variation_group <- km$cluster 

# Plot the clusters
variations_data <- as.data.frame(t(km$centers))
cluster_index <- seq(1:k)
mebn.plot_clusters(variations_data, cluster_index, assumedpredictors, assumedtargets, effects_with_most_variance, feature_index, sort_by_amount = TRUE)
```

Cholesterol medication seems to dominate the clusters. There are patients for who cholesterol medication raises the blood insulin levels more than on average and for other group the raise is less than on average.

Let's see how the clustering shows with absolute effects rather than difference from mean. Note that these are not the greatest effects, but those with most personal variance.

```{r variation clusters2, echo=FALSE, eval=TRUE, fig.width=7, fig.height=15, cache=TRUE}
source("mebn/MEBN.r")

# Number of cluster centers 
k <- 4

# Clustering using k-means
set.seed(fixed_seed)
km <- kmeans(personal_effects, centers = k)

# Every patient can be assigned to some cluster also based on how their personal reactions types

# - repeat same cluster placement for all the observations of same patient
observations_per_patient <- 4
sysdimet$effect_group <- unlist(lapply(km$cluster,rep,observations_per_patient)) 

# Plot the clusters
variations_data <- as.data.frame(t(km$centers))
#mebn.plot_clusters(variations_data, assumedpredictors, assumedtargets, largest_personal_effects, feature_index, sort_by_amount = TRUE)
cluster_index <- seq(1:k)
mebn.plot_clusters(variations_data, cluster_index, assumedpredictors, assumedtargets, effects_with_most_variance, feature_index, sort_by_amount = TRUE)
#mebn.plot_clusters(variations_data, cluster_index, assumedpredictors, assumedtargets, effects_with_most_variance, feature_index, sort_by_amount = TRUE)

```

By looking at the absolute effect of the cholesterol medication it seems that there separetes two groups where one it is a significant factor in explaining blood insulin level, and other group where it does not play virtually any role. But does these patients even take cholesterol medications? Let's create a cross tabulation of observations to explore this effect more closely.

```{r cholmed_crosstab}

table(sysdimet$kolestrolilaakitys, sysdimet$effect_group)

```
Average blood insulin levels in these clusters:

```{r}

for (i in seq(1,4)) print(paste0("cluster ", i, ": ", mean(sysdimet[sysdimet$effect_group == i,]$fsins)))

```

There are 9 (=36/4) and 7 (=28/4) patients in clusters 2 and 4 who are taking cholesterol medication. For patients in the cluster 4 the average insulin level is  the medication seems to rise the insulin level quite much, but for patients in the cluster 2 not much at all. 

Let's try to confirm this finding by visualizing the components of insulin variance.

```{r insuling_componentes_plot, echo=FALSE, eval=TRUE, fig.width=10, fig.height=6, cache=TRUE}
source("mebn/MEBN.r")

# get indexes of personal effects for fsins in variations_data vector
fsins_index <- match("fsins", assumedtargets$Name)
fsins_components_idx <- seq(nrow(assumedpredictors)*fsins_index,nrow(assumedpredictors)*(fsins_index+1)-1)

feature_index <- c(1:nrow(assumedpredictors))
cluster_index <- c(1,2,3,4)

fsins_clusters <- variations_data[fsins_components_idx,cluster_index]

# plot the effects of all nutrients for clusters fsins
mebn.plot_clusters(fsins_clusters, cluster_index, assumedpredictors, assumedtargets[assumedtargets$Name=="fsins",], NULL, feature_index, sort_by_amount = TRUE)
```
So, based on this we have an evidence that there is group on patients in this dataset who has significantly larger blood insulin values on average and a major component affecting the raise is cholesterol medication.


Both gender and cholesterol medication are unchanging factors at the dataset. Let us next remove those from clustering features and see how the clusters form based on nutritional effects only.

```{r, echo=FALSE, eval=TRUE,message=FALSE}
feature_index <- feature_index[-c(match("sukupuoli", assumedpredictors$Name), 
                                  match("kolestrolilaakitys", assumedpredictors$Name))]


personal_variations_from_mean <- matrix(NA, ncol=length(feature_index)*nrow(assumedtargets), nrow=length(levels(sysdimet$SUBJECT_ID)))
personal_effects <- matrix(NA, ncol=length(feature_index)*nrow(assumedtargets), nrow=length(levels(sysdimet$SUBJECT_ID)))

modelcache <- "BLMM_gamma/ar1/"

for (i in c(1:nrow(assumedtargets)))
{
  targetname <- as.vector(assumedtargets[i,]$Name)
  
  target_blmm <- mebn.get_localfit(paste0(modelcache,targetname))
  posterior <- extract(target_blmm, pars = c("personal_effect", "b"))

  b_blmm <- colMeans(posterior$b)
  beta_b_blmm <- colMeans(posterior$personal_effect)
  
  # Omit predictor columns here, if needed
  # - b has intercept as first column, so the index needs to be adjusted
  b_blmm <- b_blmm[,feature_index+1] 
  beta_b_blmm <- beta_b_blmm[,feature_index] 
  
  if (i == 1) {
    # variance of the intercept is omitted
    personal_variations_from_mean <- b_blmm
    personal_effects <- beta_b_blmm
  }
  else
  {
    personal_variations_from_mean <- cbind(personal_variations_from_mean, b_blmm)
    personal_effects <- cbind(personal_effects, beta_b_blmm)
  }
}
```

Number of clusters with nutritional effects only

```{r, echo=FALSE, eval=FALSE}
library(stats)
library(gridExtra)

set.seed(fixed_seed)
k.max <- 8
wss_effects <- sapply(1:k.max, function(k){kmeans(personal_effects, k, nstart=50, iter.max = 15)$tot.withinss})
wss_vars <- sapply(1:k.max, function(k){kmeans(personal_variations_from_mean, k, nstart=50, iter.max = 15)$tot.withinss})

df_vars <- data.frame(x = 1:k.max, y = wss_vars)
df_effects <- data.frame(x = 1:k.max, y = wss_effects)

plot1 <- ggplot(data=df_effects, aes(x=x, y=y, group=1)) +
  geom_line() +
  geom_point() + 
  ggtitle("Difference in absolute reaction") +
  xlab("Number of clusters") +
  ylab("Difference between clusters")

plot2 <- ggplot(data=df_vars, aes(x=x, y=y, group=1)) +
  geom_line() +
  geom_point() + 
  ggtitle("Difference from typical behavior") +
  xlab("Number of clusters") +
  ylab("Difference between clusters")

gridExtra::grid.arrange(plot1,plot2,nrow=1)
```

```{r variation clusters3, echo=FALSE, eval=TRUE, fig.width=7, fig.height=10}
source("mebn/MEBN.r")
# Number of cluster centers 
k <- 4

# Clustering using k-means
set.seed(fixed_seed)
km <- kmeans(personal_variations_from_mean, centers = k)

# Every patient can be assigned to some cluster based on how their reactions differ from average
sysdimet$variation_group <- km$cluster 

# Plot the clusters
variations_data <- as.data.frame(t(km$centers))
cluster_index <- seq(1:k)
mebn.plot_clusters(variations_data, cluster_index, assumedpredictors, assumedtargets, effects_with_most_variance, feature_index, sort_by_amount = TRUE)
```

```{r variation clusters, echo=FALSE, eval=TRUE, fig.width=10, fig.height=10}

# Number of cluster centers 
k <- 4

# Clustering using k-means
set.seed(fixed_seed)
km <- kmeans(personal_effects, centers = k)

# Every patient can be assigned to some cluster also based on how their personal reactions types

# - repeat same cluster placement for all the observations of same patient
observations_per_patient <- 4
sysdimet$effect_group <- unlist(lapply(km$cluster,rep,observations_per_patient)) 

# Plot the clusters
variations_data <- as.data.frame(t(km$centers))
cluster_index <- seq(1:k)
mebn.plot_clusters(variations_data, cluster_index, assumedpredictors, assumedtargets, effects_with_most_variance, feature_index)
#mebn.plot_clusters(variations_data, cluster_index, assumedpredictors, assumedtargets, largest_personal_effects, feature_index)

```

In clusters 1 and 2 there are patients whose blood insulin levels react on lower and on higher than average. One interesting difference is lignin's effect to the blood insulin.

TODO: Can we take one of these groups and predict if new person belongs to these reaction groups?

### Predicting personal reaction type

Finally, we can pick one the previously found reaction type clusters and hold aside the data for few of these patients. The goal would be then to use our model to predict the blood test values for this test group by their diet. This also reveals their personal reaction type for nutrition. We can compare this prediction to the true blood test values at dataset and also to the previous estimates from the whole sample population.

```{r}
table(sysdimet$effect_group)
```

For testing, we sample a holdout set of patients overall the dataset. We then train a model without this holdout set and test how accurately we can place the predicted reaction types in previously found clusters. Note that we don't know the true reaction types for these patients. This makes absolute testing of prediction performance impossible, but in this way we can see at least if the prediction is consistent with analysis of the whole dataset. We know the previous cluster assiments of these holdout patients.

```{r holdout_data, echo=FALSE, eval=TRUE, message=FALSE}
library(loo)

set.seed(fixed_seed)

#holdout_data <- sysdimet[sysdimet$variation_group==3,]
#training_data <- sysdimet[!(sysdimet$SUBJECT_ID %in% holdout_data$SUBJECT_ID),]

# Split the data in 10 folds and take 1 for holdout and rest for training
# All the observations from one patient is included in the fold
holdout_index <- kfold_split_stratified(K = 20, x = sysdimet$SUBJECT_ID)

holdout_fold <- 1

holdout_index[holdout_index == holdout_fold] <- 1
holdout_index[holdout_index != holdout_fold] <- 0

obs_per_person <- 4
holdout_persons <- holdout_index[seq(1, length(holdout_index),obs_per_person)]
holdout_persons <- which(holdout_persons %in% 1) # index of subjects
holdout_persons

```

The predictive ability of the model might be reduced by overfitting to small nuances of the whole dataset. To overcome the possible overfitting, we apply the Finnish horseshoe, a shrinkage prior, on regression coefficients. It allows specifying a prior knowledge, or at least an educated guess, about the number of significant predictors for a target. Here we guess that one third of the nutrients might be relevant for any given blood test, and apply following parameters for the shrinkage

```{r shrinkage_parameters, echo=TRUE, message=FALSE}
shrinkage_parameters <- within(list(),
{
    scale_icept  <- 1         # prior std for the intercept
    scale_global <- 0.01825   # scale for the half-t prior for tau: 
                              # ((p0=6) / (D=22-6)) * (sigma / sqrt(n=106*4))
    nu_global    <- 1         # degrees of freedom for the half-t priors for tau
    nu_local     <- 1         # degrees of freedom for the half-t priors for lambdas
    slab_scale   <- 1         # slab scale for the regularized horseshoe
    slab_df      <- 1         # slab degrees of freedom for the regularized horseshoe           
})
```

Next we fit the previous model with the Finnish horseshoe added. Besides the shrinkage parameters, we provide now the data in two sets: input data for estimation and target data to hold out for prediction. 

```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
source("mebn/MEBN.r")

initial_graph <- mebn.fully_connected_bipartite_graph(datadesc)
sysdimet_gamma_ar1_rhs_pred <- mebn.bipartite_model(reaction_graph = initial_graph, 
                                   inputdata = sysdimet,
                                   targetdata = holdout_index,
                                   predictor_columns = assumedpredictors, 
                                   assumed_targets = assumedtargets, 
                                   group_column = "SUBJECT_ID",
                                   local_estimation = mebn.sampling,
                                   local_model_cache = paste0("models/BLMM_gamma/ar1_rhs_effpred"), 
                                   stan_model_file = "mebn/BLMM_gamma_ar1_rhs_effpred.stan",
                                   reg_params = shrinkage_parameters,
                                   normalize_values = TRUE)
```

```{r gamma_ar1__effrpred_ppc, echo=FALSE, message=FALSE, warning=FALSE, eval=TRUE, cache=TRUE}
mebn.target_dens_overlays("BLMM_gamma/ar1_rhs_effpred/", assumedtargets, sysdimet[holdout_index==0,])
```

```{r}

# get variation clusters of holdout persons

sysdimet$subject_idnum <- as.numeric(substr(sysdimet$SUBJECT_ID,2,5))
sysdimet[sysdimet$subject_idnum %in% holdout_persons,]$subject_idnum

# TODO: näytä esim. ligniinin henkilökohtainen vaikutus insuliiniin ja onko se klusterin mukainen

# Näytä holdout-potilaan mittauksia ja miten reagointitapa ennuste sopii niihin (scatterplot?)

```

```{r personal_graph1}
source("mebn/MEBN.r")

# This is a predicted personal reaction graph of patient in certain cluster

# This method adds (or should add) values of b and personal nodes to a typical graph

personal_graph9 <- mebn.personal_graph(person_id = 9, 
                                       reaction_graph = sysdimet_gamma_ar1_rhs_pred, 
                                       predictor_columns = assumedpredictors, 
                                       assumed_targets = assumedtargets, 
                                       local_model_cache = "BLMM_gamma/ar1_rhs_effpred/")
```

```{r, eval=FALSE, echo=FALSE}

personal_graph9

write.graph(personal_graph9, "personal_graph_9.graphml", "graphml")

p9 <- read.graph("personal_graph_9.graphml", "graphml")
p9

```



```{r personal_graphvisu1, fig.height = 8, fig.width = 10, fig.align = "center", message=FALSE, warning=FALSE, cache=TRUE}
source("mebn/MEBN.r")
mebn.plot_personal_effects(personal_graph9, 15)
```


Compare lignin to insulin effect of two patients who were previously found to be in different clusters

```{r, echo=FALSE, eval=TRUE, cache=TRUE}
source("mebn/MEBN.r")

subject_id <- 9
lignin_id <- match("ligniini", datadesc$Name)

fsins_blmm <- mebn.get_localfit("BLMM_gamma/ar1_rhs_effpred/fsins")
posterior <- as.array(fsins_blmm)

lignin_plot <- mcmc_areas(posterior, pars = paste0("personal_effect[",subject_id,",",lignin_id,"]"), prob = 0.50, prob_outer = 0.95, point_est = "mean") + ylab("probability") + ggtitle("lignin") +
  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank())

bayesplot_grid(plots = list(lignin_plot), legends = FALSE)

```

```{r, eval=FALSE, echo=FALSE}

# Andrew Gelman, Ben Goodrich, Jonah Gabry, and Aki Vehtari (2018). R-squared for Bayesian regression models. The American Statistician
# https://doi.org/10.1080/00031305.2018.1549100
# https://avehtari.github.io/bayes_R2/bayes_R2.html (supplement notebook)

bayes_R2 <- function(y, ypred) {
  e <- -1 * sweep(ypred, 2, y)
  var_ypred <- apply(ypred, 1, var)
  var_e <- apply(e, 1, var)
  var_ypred / (var_ypred + var_e)
}

R2 <- bayes_R2(fshdl.true, na.omit(pred_post$Y_pred))
print(median(R2))
```

