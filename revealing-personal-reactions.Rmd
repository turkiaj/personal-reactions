---
title: "Revealing the Personal Effects of Nutrition with Mixed-Effect Bayesian Network"
author:
- Jari Turkia, jari.turkia@cgi.com
- University of Eastern Finland
bibliography: biblio.bib
output:
  pdf_document: default
  html_document: default
abstract: This notebook shows in detail how mixed-effect Bayesian network can be used to model the effects of nutrition. The hierarchical structure of the graphical model allows us to study the effects in both typical and personal levels. In addition we show that personal effect variations form clusters of similarly behaving patients. Finally, we show how personal graphical models can be predicted for patients that are held out from the model estimation. This shows that personal differences in nutrional effects do exist, and that they can be detected from repeated observations of food diaries and blood tests.
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = TRUE, fig.align="center")

# this allows using tikz rendering for plots with "dev=tikz"
knit_hooks$set(plot = function(x, options) {
  if ('tikz' %in% options$dev && !options$external) {
    hook_plot_tex(x, options)
  } else hook_plot_md(x, options)
})

# Fix seed for random number generator for getting consistent results in kmeans etc.
fixed_seed <- 678

# Load common MEBN package
source("mebn/MEBN.r")
```

In this work we propose a Bayesian network as an appealling way to model and predict the effects of nutrition. The Bayesian network is a directed graphical model where nodes of the graph are random variables and the edges between the nodes indicate an effect between variables. In the nutritional modelling we assign the amount of nutrients at the person's diet and the indicators of person's well-being those as random variables, and then we study the effects between them. The goal is then to find the connections for the graph that most probably describes the correct conditional relationships between nutrients and their effects.

In this article we use this method to analyze a dataset from the Sysdimet study [@pmid21901116] that contains repeated measurements of 17 nutrients, some basic information about patients (gender, medication) and blood test results. To ease the graph search, we focus only on two-level, bipartite, graphs where nutrients and personal details are assumed to affect blood tests, and only the magnitude of effect is left to be estimated. As we are interested in the personal variations of these nutritional effects we use a mixed-effect parametrization of Bayesian network [@Bae2016]. This allows us to estimate both typical and personal magnitudes of the nutritional effects with a same model.  

**Formal definition of the problem.** Let us denote the graph of interconnected nutrients and responses with $G$. We can then formulate the modeling problem as finding the graph $G$ that is the most probable given the data $D$

\begin{align}
{P}({G}|{D})
\end{align}

By using the Bayes' Rule we can be split this probability into proportions of the data likelihood of the given graph and any prior information we might have about suitable graphs 

\begin{align}
\label{prop_bayes_theorem}
{P}({G}|{D}) \propto {P}({D}|{G}) {P}({G})
\end{align}

This turns the problem to finding a graph that is most probable given data. To enforce our assumption of biologically plausible graphs, we set the graph prior \(P(G)\) to zero for all other graphs than the previously descibed bipartite graphs with nutrients affecting the bodily responses.

The joint probability of the graph \(P(G|D)\) can furthermore factorized into separate local probability distributions [@Bae2016, @Koller:2009:PGM:1795555] by their \textit{Markov blankets}. These probability distributions can be estimated separately and the novelity of our approach lies in their hierarchical mixed-effect estimation.

**Hierarchical estimation of the local distributions**. We are interested in the personal variations of how different nutrients affect. For this we need repeated measurements of nutrients in person's diet and the corresponding bodily responses. By collecting these personal sets of measurements we obtain a dataset from where we can estimate both typical and personal effects of nutrition. Modeling correctly this hierarchical structure of data should give us the best fitting model and also the parameters that explain the magnitudes of effects.

Concretely, we can use generalized linear mixed-effect models to describe the local probability distributions of the Bayesian network. We assume that every  random variable at the network can be explaned with a probability distribution from an exponential family of distributions with linear link function with input from its parent nodes. We furthermore assume that the effect between variables can be factorized into typical and personal parts. Here typical part of the effect is denoted with coefficient $\beta$ and personal part as $b$

\begin{align}
\begin{split}
\label{EXPFAM LME}
{P}({Y_i}|{pa(Y_i), \phi_i, G_i}) = {EXPFAM}({Y_i} | pa(X_i)\beta_i + pa_Z(X_i)b_i, \sigma^2)
\end{split}
\end{align}

Note that while this formulation generalizes to graphs with multiple levels, we have a prior assumption ({P}({G})) that the structure of our dataset is a bipartite graph. 

```{r data_loading, echo=FALSE, message=FALSE}

# Read the data description
datadesc <- read.csv(file="Data description.csv", header = TRUE, sep = ";")

# Read the actual data matching the description
sysdimet <- read.csv(file="data/SYSDIMET_diet.csv", sep=";", dec=",")

# Define how to iterate through the graph
assumedpredictors <- datadesc[datadesc$Order==100,]    
assumedtargets <- datadesc[datadesc$Order==200,] 
```

```{r graph, fig.height = 8, fig.width = 10, fig.align = "center", echo=FALSE, message=FALSE}
library(igraph)
initial_graph <- mebn.fully_connected_bipartite_graph(datadesc)

V(initial_graph)$size = 10 
# - put all blood test values in own rank
bipa_layout <- layout_as_bipartite(initial_graph, types = V(initial_graph)$type == "100")
# - flip layout sideways, from left to right
gap <- 6
bipa_layout <- cbind(bipa_layout[,2]*gap, bipa_layout[,1])

V(initial_graph)[V(initial_graph)$type == "100"]$label.degree = pi # left side
V(initial_graph)[V(initial_graph)$type == "200"]$label.degree = 0 # right side

plot(initial_graph,
       layout=bipa_layout, 
       rescale=TRUE,
       vertex.label.family="Helvetica",
       vertex.label.color="black",
       vertex.label.cex=1,
       vertex.label.dist=4,
       edge.arrow.size=0.5,
       edge.arrow.width=1)

```

We assume a connection from every nutrient and personal information to every blood test value at the dataset. The model search is then focused on to estimating optimal coefficients $\beta$ and $b$ that we use to define the strength of the connection. It is also important to find correct probability distributions for the random variables.

Our starting assumption is that both nutrients and the blood test values are normally distributed. This assumption is naive as it allows the blood tests to have negative values. We normalize all the input values to same scale, so that the estimated regression coefficients can be used as a indicators of connection strenght.

```{r graph_with_normal_rvs, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, cache=TRUE}
sysdimet_normal <- mebn.bipartite_model(reaction_graph = initial_graph, 
                                         inputdata = sysdimet,
                                         predictor_columns = assumedpredictors, 
                                         assumed_targets = assumedtargets, 
                                         group_column = "SUBJECT_ID",
                                         local_estimation = mebn.sampling,
                                         local_model_cache = "models/BLMM_normal", 
                                         stan_model_file = "mebn/BLMM_normal.stan",
                                         normalize_values = TRUE)

```

We can assess the model fit by plotting the posterior predictive distributions (PPC) over the distributions of the true blood test values. As expected, the Gaussian random variables are not fitting well.

```{r normal_model_ppc, fig.height = 6, fig.width = 6, fig.align = "center", echo=FALSE, eval=FALSE, message=FALSE, cache=TRUE}
# Here a PPC plot of blood insulin illustrates the problem with fitting Normal distribution to biometric data that is positive and many times right skewed. 

normal_targets <- assumedtargets[assumedtargets$Name=="fshdl",]
normal_targets$ScaleMin <- -10
normal_targets$ScaleMax <- 25

mebn.target_dens_overlays("BLMM_normal/", normal_targets, sysdimet)
```

**Developing the model**

More realistic probability distribution for blood test values would be Log-Normal or Gamma distributions [@10.1371/journal.pone.0021403]. They both allow only positive values and model better the right tail of individual larger values. For further development, we choose Gamma distribution with identity link function. This is important as it keeps the regression coefficients of the nutrients on the same absolute scale than with Normal models. The regression coefficients are used as weights of the edges at the Bayesian network and they all should be on the same scale regardless of the random variables' distribution. 

```{r graph_with_gamma_response, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE, cache=TRUE}
initial_gamma_graph <- mebn.new_graph_with_randomvariables(datadesc)

sysdimet_gamma <- mebn.bipartite_model(reaction_graph = initial_gamma_graph, 
                                   inputdata = sysdimet,
                                   predictor_columns = assumedpredictors, 
                                   assumed_targets = assumedtargets, 
                                   group_column = "SUBJECT_ID",
                                   local_estimation = mebn.sampling,
                                   local_model_cache = "models/BLMM_gamma/hierarchical_idlink", 
                                   stan_model_file = "mebn/BLMM_gamma_hierarchical.stan",
                                   normalize_values = TRUE)
```

The visual comparison of the true and estimated distributions of blood test variables shows that Gamma distribution follows the true distributions quite well. The estimate has lots of variance and we should focus on that next.


```{r gamma_ppc, echo=FALSE, eval=TRUE, message=FALSE, cache=TRUE}
mebn.target_dens_overlays("BLMM_gamma/hierarchical_idlink/", assumedtargets, sysdimet)
```

```{r effect_comparison, eval=FALSE}
#As we see here, the regression coefficients are approximately the same for both Normal and Gamma models, but the confidence is better for the Gamma model 

beta_coefs <- mebn.compare_typicals(sysdimet_normal, sysdimet_gamma)
head(beta_coefs)
```

**Comparing different local models with LOO**

Besides the visual inspection, we can also compare the models using LOO-PSIS information criteria. It uses log probability that is calculated as part of the Stan models. We can approximate the probability of the whole graph by summing over the expected log predictive densities (ELPD) of every distict local distribution at the graph.

This shows how the local distributions with Normal and Gamma distribution compare to each other

```{r, normal_gamma_loo, message=FALSE, warning=FALSE, cache=TRUE}
normal_vs_gamma <- mebn.LOO_comparison(assumedtargets, "BLMM_normal", "BLMM_gamma/hierarchical_idlink")
normal_vs_gamma
# https://haozhu233.github.io/kableExtra/awesome_table_in_html.html
```

As ELPD of the gamma model is higher, we should choose that for further development.

##Varying response time: AR-structures##

In the dataset the observations from patients' food diaries and blood tests have one week response time. This might not be optimal time for all the responses and the successive observations might be correlated. For modeling the correlated residuals, we add an autocorrelation structure to the model. As there are only four observations from each patient, we consider only one previous residual for each observation.

In Stan model this AR(1) structure is calculated as follows

```
...
    // - calculate residuals     
    e[n] = Y[n] - mu[n]; 

    // - estimate autoregression coefficient for observations of one patient 
    if (group_size > 1)
      mu[n] += e[n-1] * ar1;
      
...      
```

```{r graph_with_gamma_ar1, echo=FALSE, eval=TRUE, message=FALSE, warning=FALSE, cache=TRUE}
initial_graph <- mebn.new_graph_with_randomvariables(datadesc)
sysdimet_gamma_ar1 <- mebn.bipartite_model(reaction_graph = initial_graph, 
                                   inputdata = sysdimet,
                                   predictor_columns = assumedpredictors, 
                                   assumed_targets = assumedtargets, 
                                   group_column = "SUBJECT_ID",
                                   local_estimation = mebn.sampling,
                                   local_model_cache = "models/BLMM_gamma/ar1", 
                                   stan_model_file = "mebn/BLMM_gamma_ar1.stan",
                                   normalize_values = TRUE)

mebn.write_gexf(sysdimet_gamma_ar1, "sysdimet_gamma_ar1.gexf")
write_graph(sysdimet_gamma_ar1, "sysdimet_gamma_ar1.graphml", "graphml")

```

The autocorrelation structure seems to decrease the variance of the model

```{r gamma_ar1_ppc, echo=FALSE, message=FALSE, warning=FALSE, eval=TRUE, cache=TRUE}
mebn.target_dens_overlays("BLMM_gamma/ar1/", assumedtargets, sysdimet)
```

Numerically the difference is also significant between models with AR(0) and AR(1)

```{r gamma_ar1_loo, message=FALSE, warning=FALSE, cache=TRUE}
ar0_vs_ar1 <- mebn.LOO_comparison(assumedtargets, "BLMM_gamma/hierarchical_idlink", "BLMM_gamma/ar1")
ar0_vs_ar1
```

It is interesting also to compare the AR(1) coefficients of different local distributions at the graph. For HDL cholesterol (fshdl), combined cholesterol (fskol) and the blood glucose (fpgluk) the positive AR(1) coefficient indicates that the effect of nutrition spans longer than one week 

```{r ar1_comparison, message=FALSE, warning=FALSE, cache=TRUE}
ar1_comparison <- mebn.AR_comparison(assumedtargets, "BLMM_gamma/ar1")
ar1_comparison
   
```

#Typical effects of nutrition

We have now reached a reasonable good fit for the local distributions of the Bayesian network. Our code has extracted a graphical model from these  posterior distributions of random variables and parameters. The graph consists mean values of the posteriors and also their credible intervals. This is a lighter data structure for further analysis and allows also graph operations besides the regression modeling. 

Besides the random variables, the graph also includes the regression coefficients $\beta$ and $b$ that denote the typical and personal magnitudes of the effects for different nutrients. For better overview, let us plot the graph of typical nutritional effects. This visualization shows typical coefficients $\beta$ as weight of the connection between blood test and nutrients at diet affecting it. For clarity, the visualization only shows 15 principal components explaining the variance of each blood test

```{r, typical_effects_figure, fig.height = 8, fig.width = 10, fig.align = "center", message=FALSE, warning=FALSE, cache=TRUE}
mebn.plot_typical_effects(sysdimet_gamma_ar1, 15)
```

#Personal effects

Beside the typical effects, our main interest lies on the personal effects $b$ and their variances $\sigma_b$. If there is a large variance for the personal effect it means that there exists personal differences in reaction types. These differences are our main interest to find.

When this dataset is arranged as a bipartite graph, there are 22*5=110 possible effects between nutrients and blood tests. For finding the most interesting effects with most personal variance, we extract and sort b_sigma-hyperparameters from the graph.

```{r, echo=FALSE, message=FALSE, eval=TRUE}
library(igraph)
#sysdimet_gamma_ar10 <- mebn.read_gexf("sysdimet_gamma_ar1.gexf") 
sysdimet_gamma_ar1 <- read_graph("sysdimet_gamma_ar1.graphml", "graphml")

# Query the graph for personal variances, denoted by b_sigma-nodes
allnodes <- V(sysdimet_gamma_ar1)
b_sigma <- allnodes[allnodes$type=="b_sigma"]
#beta_nodes <- allnodes[allnodes$type=="beta"]

# Again, a separate data frame is constructed for printing
personal_variances<-data.frame(matrix(NA, nrow=length(b_sigma), ncol=0))

personal_variances$effect <- unlist(lapply(strsplit(gsub("b_sigma_","", b_sigma$name), "_"), function(x) paste0(toString(datadesc[datadesc$Name==x[1],]$Description)," -> ", toString(datadesc[datadesc$Name==x[2],]$Description))))

personal_variances$variance <- b_sigma$value
personal_variances$"CI-10%" <- b_sigma$value_lCI
personal_variances$"CI-90%" <- b_sigma$value_uCI

ordered_personal_variance <- personal_variances[order(-personal_variances$variance),]

head(ordered_personal_variance, 20)
```

These are the effect with most difference between patients. Let us the examine closer those of the effects that have most probable variance over 0.30 and plot them as a graph

```{r personal_variations_figure, fig.height = 8, fig.width = 10, fig.align = "center", cache = TRUE}
effects_with_most_variance <- ordered_personal_variance[ordered_personal_variance$variance >= 0.30,]
number_of_varying_effects <- nrow(effects_with_most_variance)

mebn.plot_personal_variations(sysdimet_gamma_ar1, number_of_varying_effects)
```

##Clusters of personal graphs

Although there are personal differences, it is likely that everyone is not behaving uniquely but there might exist similar groups of behavior. We can analyze personal differences in two ways. We can look the absolute magnitudes of effects and we can also look how persons differ from typical behavior or mean of the effect.

We can now take the personal estimations of the effects and see, if they form clusters

```{r kmeans_dfs, echo=FALSE, cache=FALSE, message=FALSE}
library(rstan)

# Pick these predictors as features for clustering -- get all
feature_index <- c(1:nrow(assumedpredictors))

personal_variations_from_mean <- matrix(NA, ncol=length(feature_index)*nrow(assumedtargets), nrow=length(levels(sysdimet$SUBJECT_ID)))
personal_effects <- matrix(NA, ncol=length(feature_index)*nrow(assumedtargets), nrow=length(levels(sysdimet$SUBJECT_ID)))

modelcache <- "BLMM_gamma/ar1/"

# We could fetch a personal graph for all patients, but it is more effective to extract estimations directly from MCMC-samples to data frames
# TODO: use mebn.personal_effects. Do we need CIs here?


for (i in c(1:nrow(assumedtargets)))
{
  targetname <- as.vector(assumedtargets[i,]$Name)
  
  target_blmm <- mebn.get_localfit(paste0(modelcache,targetname))
  posterior <- extract(target_blmm, pars = c("personal_effect", "b"))

  b_blmm <- colMeans(posterior$b)
  beta_b_blmm <- colMeans(posterior$personal_effect)
  
  # Omit predictor columns here, if needed
  # - b has intercept as first column, so the index needs to be adjusted
  b_blmm <- b_blmm[,feature_index+1] 
  beta_b_blmm <- beta_b_blmm[,feature_index] 
  
  if (i == 1) {
    # variance of the intercept is omitted
    personal_variations_from_mean <- b_blmm
    personal_effects <- beta_b_blmm
  }
  else
  {
    personal_variations_from_mean <- cbind(personal_variations_from_mean, b_blmm)
    personal_effects <- cbind(personal_effects, beta_b_blmm)
  }
}

```

```{r personal_effects, echo=FALSE, cache=FALSE}

# Pull effects form all patients to one vector
nperson <- 106
largest_personal_effects <- as.data.frame(as.vector(personal_effects))
colnames(largest_personal_effects) <- c("amount")
largest_personal_effects$abs_amount <- abs(largest_personal_effects$amount)
largest_personal_effects$predictor <- rep(rep(assumedpredictors[feature_index,]$Description,nrow(assumedtargets)), nperson)

cl <- rep(1,length(feature_index)) # number of predictors
t_idx <- c(cl,cl+1,cl+2,cl+3,cl+4) # predictors x targets
largest_personal_effects$response <- rep(assumedtargets$Description[t_idx], nperson)

largest_personal_effects$effect <- paste0(largest_personal_effects$predictor," -> ", largest_personal_effects$response)

largest_personal_effects <- largest_personal_effects[largest_personal_effects$abs_amount > 0,]
largest_personal_effects <- largest_personal_effects[order(-largest_personal_effects$abs_amount),]
largest_personal_effects <- largest_personal_effects[c("effect", "amount")]


```

## Latent grouping

```{r, echo=FALSE, eval=TRUE}
library(stats)
library(gridExtra)

set.seed(fixed_seed)
k.max <- 8
wss_effects <- sapply(1:k.max, function(k){kmeans(personal_effects, k, nstart=50, iter.max = 15)$tot.withinss})
wss_vars <- sapply(1:k.max, function(k){kmeans(personal_variations_from_mean, k, nstart=50, iter.max = 15)$tot.withinss})

df_vars <- data.frame(x = 1:k.max, y = wss_vars)
df_effects <- data.frame(x = 1:k.max, y = wss_effects)

plot1 <- ggplot(data=df_effects, aes(x=x, y=y, group=1)) +
  geom_line() +
  geom_point() + 
  ggtitle("Difference in absolute reaction") +
  xlab("Number of clusters") +
  ylab("Difference between clusters")

plot2 <- ggplot(data=df_vars, aes(x=x, y=y, group=1)) +
  geom_line() +
  geom_point() + 
  ggtitle("Difference from typical behavior") +
  xlab("Number of clusters") +
  ylab("Difference between clusters")

gridExtra::grid.arrange(plot1,plot2,nrow=1)
```

By looking previous diagram we see that there are four clearly identifiable groups between patients. At this plot, zero of X-axis denotes a typical behaviour of that particular reaction, and blue and red bars denote a deviation from this typical mean.

```{r variation clusters1, echo=FALSE, eval=TRUE, fig.width=7, fig.height=10}
source("mebn/MEBN.r")

# Number of cluster centers 
k <- 4

# Clustering using k-means
set.seed(fixed_seed)
km <- kmeans(personal_variations_from_mean, centers = k)

# Every patient can be assigned to some cluster based on how their reactions differ from average
sysdimet$variation_group <- km$cluster 

# Plot the clusters
variations_data <- as.data.frame(t(km$centers))
cluster_index <- seq(1:k)
mebn.plot_clusters(variations_data, cluster_index, assumedpredictors, assumedtargets, effects_with_most_variance, feature_index, sort_by_amount = TRUE)
```

Cholesterol medication seems to dominate the clusters. There are patients for who cholesterol medication raises the blood insulin levels more than on average and for other group the raise is less than on average.

Let's see how the clustering shows with absolute effects rather than difference from mean. Note that these are not the greatest effects, but those with most personal variance.

```{r variation clusters2, echo=FALSE, eval=TRUE, fig.width=8, fig.height=10, cache=TRUE}
source("mebn/MEBN.r")

# Number of cluster centers 
k <- 4

# Clustering using k-means
set.seed(fixed_seed)
km <- kmeans(personal_effects, centers = k)

# Every patient can be assigned to some cluster also based on how their personal reactions types

# - repeat same cluster placement for all the observations of same patient
observations_per_patient <- 4
sysdimet$effect_group <- unlist(lapply(km$cluster,rep,observations_per_patient)) 

# Plot the clusters
variations_data <- as.data.frame(t(km$centers))
#mebn.plot_clusters(variations_data, assumedpredictors, assumedtargets, largest_personal_effects, feature_index, sort_by_amount = TRUE)
cluster_index <- seq(1:k)
mebn.plot_clusters(variations_data, cluster_index, assumedpredictors, assumedtargets, effects_with_most_variance, feature_index, sort_by_amount = TRUE)
#mebn.plot_clusters(variations_data, cluster_index, assumedpredictors, assumedtargets, effects_with_most_variance, feature_index, sort_by_amount = TRUE)

```

By looking at the absolute effect of the cholesterol medication it seems that there separetes two groups where one it is a significant factor in explaining blood insulin level, and other group where it does not play virtually any role. But does these patients even take cholesterol medications? Let's create a cross tabulation of observations to explore this effect more closely.

```{r cholmed_crosstab, fig.width=8, fig.height=1}

table(sysdimet$kolestrolilaakitys, sysdimet$effect_group)

```
This indicates that patients in clusters 2 and 4 are indeed taking cholesterol medication.

The average blood insulin levels in these clusters are

```{r}
ffor (i in seq(1,4)) print(paste0("cluster ", i, ": ", mean(sysdimet[sysdimet$effect_group == i,]$fsins)))
```

There are 9 (=36/4) and 7 (=28/4) patients in clusters 2 and 4 who are taking cholesterol medication. For patients in the cluster 4 the average insulin level is  the medication seems to rise the insulin level quite much, but for patients in the cluster 2 not much at all. 

Let's try to confirm this finding by visualizing the components of insulin variance.

```{r insuling_componentes_plot, echo=FALSE, eval=TRUE, fig.width=8, fig.height50, cache=TRUE}
source("mebn/MEBN.r")

# get indexes of personal effects for fsins in variations_data vector
fsins_index <- match("fsins", assumedtargets$Name)
fsins_components_idx <- seq(nrow(assumedpredictors)*fsins_index,nrow(assumedpredictors)*(fsins_index+1)-1)

feature_index <- c(1:nrow(assumedpredictors))
cluster_index <- c,,4))

fsins_clusters <- variations_data[fsins_components_idx,cluster_index]

# plot the effects of all nutrients for clusters fsins
mebn.plot_clusters(fsins_clusters, cluster_index, assumedpredictors, assumedtargets[assumedtargets$Name=="fsins",], NULL, feature_index, sort_by_amount = TRUE)
```
So, based on this we have an evidence that there is group on patients in this dataset who has significantly larger blood insulin values on average and a major component affecting the raise is cholesterol medication.


Both gender and cholesterol medication are unchanging factors at the dataset. Let us next remove those from clustering features and see how the clusters form based on nutritional effects only.

```{r, echo=FALSE, eval=TRUE,message=FALSE}
feature_index <- feature_index[-c(match("sukupuoli", assumedpredictors$Name), 
                                  match("kolestrolilaakitys", assumedpredictors$Name))]


personal_variations_from_mean <- matrix(NA, ncol=length(feature_index)*nrow(assumedtargets), nrow=length(levels(sysdimet$SUBJECT_ID)))
personal_effects <- matrix(NA, ncol=length(feature_index)*nrow(assumedtargets), nrow=length(levels(sysdimet$SUBJECT_ID)))

modelcache <- "BLMM_gamma/ar1/"

for (i in c(1:nrow(assumedtargets)))
{
  targetname <- as.vector(assumedtargets[i,]$Name)
  
  target_blmm <- mebn.get_localfit(paste0(modelcache,targetname))
  posterior <- extract(target_blmm, pars = c("personal_effect", "b"))

  b_blmm <- colMeans(posterior$b)
  beta_b_blmm <- colMeans(posterior$personal_effect)
  
  # Omit predictor columns here, if needed
  # - b has intercept as first column, so the index needs to be adjusted
  b_blmm <- b_blmm[,feature_index+1] 
  beta_b_blmm <- beta_b_blmm[,feature_index] 
  
  if (i == 1) {
    # variance of the intercept is omitted
    personal_variations_from_mean <- b_blmm
    personal_effects <- beta_b_blmm
  }
  else
  {
    personal_variations_from_mean <- cbind(personal_variations_from_mean, b_blmm)
    personal_effects <- cbind(personal_effects, beta_b_blmm)
  }
}
```

Number of clusters with nutritional effects only

```{r, echo=FALSE, eval=FALSE}
library(stats)
library(gridExtra)

set.seed(fixed_seed)
k.max <- 8
wss_effects <- sapply(1:k.max, function(k){kmeans(personal_effects, k, nstart=50, iter.max = 15)$tot.withinss})
wss_vars <- sapply(1:k.max, function(k){kmeans(personal_variations_from_mean, k, nstart=50, iter.max = 15)$tot.withinss})

df_vars <- data.frame(x = 1:k.max, y = wss_vars)
df_effects <- data.frame(x = 1:k.max, y = wss_effects)

plot1 <- ggplot(data=df_effects, aes(x=x, y=y, group=1)) +
  geom_line() +
  geom_point() + 
  ggtitle("Difference in absolute reaction") +
  xlab("Number of clusters") +
  ylab("Difference between clusters")

plot2 <- ggplot(data=df_vars, aes(x=x, y=y, group=1)) +
  geom_line() +
  geom_point() + 
  ggtitle("Difference from typical behavior") +
  xlab("Number of clusters") +
  ylab("Difference between clusters")

gridExtra::grid.arrange(plot1,plot2,nrow=1)
```

```{r variation clusters3, echo=FALSE, eval=TRUE, fig.width=7, fig.height=10}
source("mebn/MEBN.r")
# Number of cluster centers 
k <- 4

# Clustering using k-means
set.seed(fixed_seed)
km <- kmeans(personal_variations_from_mean, centers = k)

# Every patient can be assigned to some cluster based on how their reactions differ from average
sysdimet$variation_group <- km$cluster 

# Plot the clusters
variations_data <- as.data.frame(t(km$centers))
cluster_index <- seq(1:k)
mebn.plot_clusters(variations_data, cluster_index, assumedpredictors, assumedtargets, effects_with_most_variance, feature_index, sort_by_amount = TRUE)
```

```{r variation clusters, echo=FALSE, eval=TRUE, fig.width=8, fig.height80}

# Number of cluster centers 
k <- 4

# Clustering using k-means
set.seed(fixed_seed)
km <- kmeans(personal_effects, centers = k)

# Every patient can be assigned to some cluster also based on how their personal reactions types

# - repeat same cluster placement for all the observations of same patient
observations_per_patient <- 4
sysdimet$effect_group <- unlist(lapply(km$cluster,rep,observations_per_patient)) 

# Plot the clusters
variations_data <- as.data.frame(t(km$centers))
cluster_index <- seq(1:k)
mebn.plot_clusters(variations_data, cluster_index, assumedpredictors, assumedtargets, effects_with_most_variance, feature_index)
#mebn.plot_clusters(variations_data, cluster_index, assumedpredictors, assumedtargets, largest_personal_effects, feature_index)

```

In clusters 1 and 2 there are patients whose blood insulin levels react on lower and on higher than average. One interesting difference is lignin's effect to the blood insulin.

### Predicting personal reaction type

Finally, we can pick one the previously found reaction type clusters and hold aside the data for few of these patients. The goal would be then to use our model to predict the blood test values for this test group by their diet. This also reveals their personal reaction type for nutrition. We can compare this prediction to the true blood test values at dataset and also to the previous estimates from the whole sample population.

```{r}
table(sysdimet$effect_group)
```

For testing, we sample a holdout set of patients overall the dataset. We then train a model without this holdout set and test how accurately we can place the predicted reaction types in previously found clusters. Note that we don't know the true reaction types for these patients. This makes absolute testing of prediction performance impossible, but in this way we can see at least if the prediction is consistent with analysis of the whole dataset. We know the previous cluster assiments of these holdout patients.

```{r holdout_data, echo=FALSE, eval=TRUE, message=FALSE}
library(loo)

set.seed(fixed_seed)

#holdout_data <- sysdimet[sysdimet$variation_group==3,]
#training_data <- sysdimet[!(sysdimet$SUBJECT_ID %in% holdout_data$SUBJECT_ID),]

# Split the data in 10 folds and take 1 for holdout and rest for training
# All the observations from one patient is included in the fold
holdout_index <- kfold_split_stratified(K = 20, x = sysdimet$SUBJECT_ID)

holdout_fold <- 1

holdout_index[holdout_index == holdout_fold] <- 1
holdout_index[holdout_index != holdout_fold] <- 0

obs_per_person <- 4
holdout_persons <- holdout_index[seq(1, length(holdout_index),obs_per_person)]
holdout_persons <- which(holdout_persons %in% 1) # index of subjects
holdout_persons
```

```{r mebn_personal_effects}
lignin_fsins <- largest_personal_effects[largest_personal_effects$effect=="lignin -> blood insulin",]
lignin_fsins[order(-lignin_fsins$amount),]

# TODO: Etsi potilaat, jotka reagoivat näin eri tavoin ja piirrä heidän verkkonsa sekä tämän reagoinnin posterior

# personal_effects-data framessa jokainen rivi vastaa subject_id:ta
# nyt siitä pitäisi hakea ääriarvot kuten "largest_personal_effects""
personal_effects

# holdout_persons = 9 45 50 52 63 95
#subject 9:n reagoinnit..

personal_effects[9,]
# TODO: Etsi reagoinnin "lignin -> blood insulin" arvo ja hae top 6 absoluuttisen reagoinnin (+ ja -) perusteella holdoutiin


#4790	lignin -> blood insulin	3.5055829938		
#5560	lignin -> blood insulin	1.2615230525		
#5120	lignin -> blood insulin	1.1798573236		
#4900	lignin -> blood insulin	1.0028292724		
#5780	lignin -> blood insulin	0.4125861690	

#4680	lignin -> blood insulin	-0.9430896940		
#2480	lignin -> blood insulin	-0.9790195630		
#5670	lignin -> blood insulin	-1.3028765423		
#5230	lignin -> blood insulin	-1.3556803319	

```



The predictive ability of the model might be reduced by overfitting to small nuances of the whole dataset. To overcome the possible overfitting, we apply the Finnish horseshoe, a shrinkage prior, on regression coefficients. It allows specifying a prior knowledge, or at least an educated guess, about the number of significant predictors for a target. Here we guess that one third of the nutrients might be relevant for any given blood test, and apply following parameters for the shrinkage

```{r shrinkage_parameters, echo=TRUE, message=FALSE}
shrinkage_parameters <- within(list(),
{
    scale_icept  <- 1         # prior std for the intercept
    scale_global <- 0.01825   # scale for the half-t prior for tau: 
                              # ((p0=6) / (D=22-6)) * (sigma / sqrt(n=106*4))
    nu_global    <- 1         # degrees of freedom for the half-t priors for tau
    nu_local     <- 1         # degrees of freedom for the half-t priors for lambdas
    slab_scale   <- 1         # slab scale for the regularized horseshoe
    slab_df      <- 1         # slab degrees of freedom for the regularized horseshoe           
})
```

Next we fit the previous model with the Finnish horseshoe added. Besides the shrinkage parameters, we provide now the data in two sets: input data for estimation and target data to hold out for prediction. 

```{r, echo=TRUE, eval=TRUE, message=FALSE, warning=FALSE}
source("mebn/MEBN.r")

initial_graph <- mebn.fully_connected_bipartite_graph(datadesc)
sysdimet_gamma_ar1_rhs_pred <- mebn.bipartite_model(reaction_graph = initial_graph, 
                                   inputdata = sysdimet,
                                   targetdata = holdout_index,
                                   predictor_columns = assumedpredictors, 
                                   assumed_targets = assumedtargets, 
                                   group_column = "SUBJECT_ID",
                                   local_estimation = mebn.sampling,
                                   local_model_cache = paste0("models/BLMM_gamma/ar1_rhs_effpred"), 
                                   stan_model_file = "mebn/BLMM_gamma_ar1_rhs_effpred.stan",
                                   reg_params = shrinkage_parameters,
                                   normalize_values = TRUE)
```

```{r gamma_ar1__effrpred_ppc, echo=FALSE, message=FALSE, warning=FALSE, eval=TRUE, cache=TRUE}
mebn.target_dens_overlays("BLMM_gamma/ar1_rhs_effpred/", assumedtargets, sysdimet[holdout_index==0,])
```

```{r}

# get variation clusters of holdout persons

sysdimet$subject_num <- as.numeric(substr(sysdimet$SUBJECT_ID, 2, 4))
sysdimet[sysdimet$subject_num %in% holdout_persons,]$effect_group

holdout_persons
# TODO: näytä esim. ligniinin henkilökohtainen vaikutus insuliiniin ja onko se klusterin mukainen

# Näytä holdout-potilaan mittauksia ja miten reagointitapa ennuste sopii niihin (scatterplot?)

```

```{r personal_graph1}
source("mebn/MEBN.r")

# This is a predicted personal reaction graph of patient in certain cluster

# This method adds (or should add) values of b and personal nodes to a typical graph

personal_graph9 <- mebn.personal_graph(person_id = 9, 
                                       reaction_graph = sysdimet_gamma_ar1_rhs_pred, 
                                       predictor_columns = assumedpredictors, 
                                       assumed_targets = assumedtargets, 
                                       local_model_cache = "BLMM_gamma/ar1_rhs_effpred/")
```

```{r, eval=FALSE, echo=FALSE}
library(igraph)
#write.graph(personal_graph9, "personal_graph_9.graphml", "graphml")

p9 <- read.graph("personal_graph_9.graphml", "graphml")
 
e <- E(p9)[4]
e$weight
edge_attr(p9, "b",581)
?get.edge.attribute

E(p9)$weight
E(p9)$mean

e$b
```



```{r personal_graphvisu1, eval=FALSE, fig.height = 8, fig.width = 10, fig.align = "center", message=FALSE, warning=FALSE, cache=TRUE}
source("mebn/MEBN.r")
#mebn.plot_personal_effects(p9, 15)
mebn.plot_personal_effects(personal_graph9, 15)
```


Compare lignin to insulin effect of two patients who were previously found to be in different clusters

```{r, echo=FALSE, eval=TRUE, cache=TRUE}
source("mebn/MEBN.r")

subject_id <- 9
lignin_id <- match("ligniini", datadesc$Name)

fsins_blmm <- mebn.get_localfit("BLMM_gamma/ar1_rhs_effpred/fsins")
posterior <- as.array(fsins_blmm)

lignin_plot <- mcmc_areas(posterior, pars = paste0("personal_effect[",subject_id,",",lignin_id,"]"), prob = 0.50, prob_outer = 0.95, point_est = "mean") + ylab("probability") + ggtitle("lignin") +
  theme(axis.text.y = element_blank(), axis.ticks.y = element_blank())

bayesplot_grid(plots = list(lignin_plot), legends = FALSE)

```

```{r, eval=FALSE, echo=FALSE}

# Andrew Gelman, Ben Goodrich, Jonah Gabry, and Aki Vehtari (2018). R-squared for Bayesian regression models. The American Statistician
# https://doi.org/10.1080/00031305.2018.1549100
# https://avehtari.github.io/bayes_R2/bayes_R2.html (supplement notebook)

bayes_R2 <- function(y, ypred) {
  e <- -1 * sweep(ypred, 2, y)
  var_ypred <- apply(ypred, 1, var)
  var_e <- apply(e, 1, var)
  var_ypred / (var_ypred + var_e)
}

R2 <- bayes_R2(fshdl.true, na.omit(pred_post$Y_pred))
print(median(R2))
```

